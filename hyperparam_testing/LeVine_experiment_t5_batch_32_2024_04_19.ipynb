{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1404d9-17a0-4416-9c38-894606dfd96a",
   "metadata": {},
   "source": [
    "# Thesis Experiment: t5 Model\n",
    "## Michael LeVine, April 19, 2024\n",
    "## Hyperparameter testing: batch size 32\n",
    "\r\n",
    "The purpose of this notebook is to test the summarization capabilities of the t5 mode when fine-tuning it with a batch size of 32l.\r\n",
    "\r\n",
    "Attribution: This approach is based on a training course of Janana Ravi, Certified Google Cloud Architect and Data Engineer - from the LinkedIn Learning course: AI Text Summarization with Hugging Face, released 10/30/2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c0d0a-947a-4c12-b010-02a8dfec00d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview: Using a Transformer Model from Hugging Face: t5\n",
    "\n",
    "### The t5 model\n",
    "The pre-trained model that we will use is the \"t5-small\" model from Hugging Face, which can be found here: https://huggingface.co/google-t5/t5-small\n",
    "\n",
    "The \"model card,\" which describes the model, notes that it is a \"text-to-text\" transformer model.  The model card notes that t5 base is the model checkpoint with 60 million parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953359f-3ee3-4485-aab0-29e831fca60f",
   "metadata": {},
   "source": [
    "## Verifying the Compute Environment\n",
    "\n",
    "### Graphics Processing Unit (GPU)\n",
    "Running inference on transformer models can be done without a GPU.  However, for training, a GPU is recommended.  The following block of code shows checks whether a GPU is available for use in a PyTorch environment. \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1298c3c-87c1-474f-880b-5b4f580fc0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"using\", device, \"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe6afa-64cf-4d44-bc74-6a3d1f7e498b",
   "metadata": {},
   "source": [
    "## Installing and importing required libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b87d038-2d03-479e-912a-78cbc126da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#command line pip install the necessary required libraries and dependencies\n",
    "#the transformers library allows us to access the pre-trained t5-small model\n",
    "#the datasets library provides access to the Hugging Face datasets\n",
    "#the evaluate model enables us to evaluate the summarizations the model produces\n",
    "#the rouge_score is a standard evaluation metric used in text summarization tasks\n",
    "#the accelerate function allows for distributed training on GPUs\n",
    "#pip install transformers datasets evaluate rouge_score accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a5ed-e53c-4bcd-b774-b15acc1078c0",
   "metadata": {},
   "source": [
    "### Import the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0208b42b-c0c7-448c-98f4-ca6aac89c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.32.1\n"
     ]
    }
   ],
   "source": [
    "#this code imports needed libraries\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "import rouge_score\n",
    "import accelerate\n",
    "\n",
    "print(transformers.__version__) # verifies the transformers version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72786461-ca9f-4cdf-9233-8bcd9eecb5a6",
   "metadata": {},
   "source": [
    "## Importing, Reducing, and Exploring the dataset\n",
    "\n",
    "The experiment will use the CNN/Daily Mail dataset.  Two datasets will be created:\n",
    "* Training Dataset\n",
    "* Holdout Dataset (for running inference on the model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527a6a3-585b-4607-af93-5cdcf3b849f4",
   "metadata": {},
   "source": [
    "### Instantiating a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728bf2b-6038-416d-8018-88888df75248",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/home/anthony/Michael LeVine/data'\n",
    "file_name = 'your_data_file.csv'  # Replace with your actual file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca83f6d-516a-4373-aabb-dd4f75b1c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0144c4e70a6e45baa79910d716b5da89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e4dfb06d194283ba37084cf86a0e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading the dataset which was previously saved\n",
    "from datasets import load_dataset\n",
    "cnn_news_summary_ds = load_dataset(\"arrow\", data_files={'train': 'data/cnn_news_summary_ds/train/data-00000-of-00001.arrow', 'test': 'data/cnn_news_summary_ds/test/data-00000-of-00001.arrow'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a8ce64-2a55-439d-ae3b-3bde3b39a883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 2296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_news_summary_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c640184-0779-44b5-89b1-5f7e7356f1a0",
   "metadata": {},
   "source": [
    "As per above output, now the dataset is broken down into two components:\n",
    "* a `train` (training) dataset of 2296 articles\n",
    "* a `test` dataset of 575\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd57214-d280-48c9-8071-2c9927d2511b",
   "metadata": {},
   "source": [
    "### Instantiating a holdout dataset (200 records)\n",
    "\n",
    "The holdout dataset is used to run inference on both the \"off-the-shelf\" model and the fine-tuned model.  The purpose of having a holdout set is so the model is running inference on a different dataset from what it was trained on in order to test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4787d10-36f0-4fff-9f28-00aa2bd5dd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0489594b4e04479982ee217ab1400e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load holdout set for inference from a local csv (to ensure same order)\n",
    "cnn_holdout_ds = load_dataset (\"csv\", data_files='data/cnn_holdout_ds.csv', split = \"train[0:200]\")\n",
    "cnn_holdout_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e027c7-158b-4fcc-b596-ae3aaff89a8b",
   "metadata": {},
   "source": [
    "### Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82e2590-2499-4f3e-a36a-0bffe1857c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (2296, 3), 'test': (575, 3)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset shape\n",
    "cnn_news_summary_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ae909-4ae4-49e7-853a-2bd9bcf94fc3",
   "metadata": {},
   "source": [
    "The above output shows the cnn_dailymail `train` set is 2296 rows x 3 columns.  The `test` set is 575 rows x 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f20936f-3c88-44df-80f2-eaa27635a316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset object type\n",
    "type(cnn_news_summary_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a2d68-b727-4c7a-abfc-25448cf52e96",
   "metadata": {},
   "source": [
    "The above output shows the cnn_dailymail dataset is of type Dataset, within the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b1b6cc-ffbe-41f7-ac42-cdfd3015e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 2296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset structure\n",
    "cnn_news_summary_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53377013-fe0d-4813-93dd-600ebd583b78",
   "metadata": {},
   "source": [
    "The Dataset has three features: \n",
    "* `article`: The full text of the news article\n",
    "*  `highlights`: the target summary, also known as the reference summary\n",
    "*  `id`: the unique id for each article/highlights pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a06cfd-c6d2-4907-a506-7136f9093461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': Value(dtype='string', id=None),\n",
       " 'highlights': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the features of the dataset\n",
    "cnn_news_summary_ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0ea01f-29c0-483d-9199-c07872a0ec40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': '(CNN) -- After almost 10 months, the FBI has zeroed in on a suspect in the case of missing Florida pilot Robert Wiles, who may have been kidnapped for ransom. Missing Florida pilot Robert Wiles is thought to have been kidnapped for ransom. \"We\\'re close to solving the case,\" said FBI special agent David Couvertier. He would not elaborate. Agents also would not identify the suspect, and they said the person is not in custody. Investigators would only reveal that the \"key suspect\" is in Florida, either in Orlando, Lakeland or Melbourne. \"They\\'re holding that back in hopes of getting additional information,\" said Couvertier. The FBI says it\\'s also looking at several persons of interest in those same three Florida cities. Wiles, 27, was last seen in the family\\'s aircraft maintenance business, National Flight Services, at Lakeland Linder Regional Airport on April 1, 2008. The day Wiles disappeared, he left behind his bags, his computer, and even his car. His father says the next day, Wiles was supposed to be on a flight out of Orlando. He never showed up. Two days after he vanished, Wiles\\' father, Thomas, received a ransom note. It demanded money and threatened to harm Robert Wiles if the terms weren\\'t met. Wiles\\' parents said they tried to comply but heard nothing back. Nearly a year later, the FBI says their investigation shows that those involved \"were very familiar with Robert\\'s work, Robert\\'s personal information and had knowledge of his parent\\'s personal affairs.\" Agents say they\\'ve tracked down leads in nine U.S. cities and as far away as Thailand where National Flight Services also does business. The Ohio-based company services aircraft in 50 countries and has been in business since 1972. Investigators are reaching out to the public in hopes of sparking any additional information about Wiles to \"close the loop,\" Couvertier said. Investigators want to talk with \"people who knew about his work routine, who might know someone who was obsessed with him, who was upset or jealous of him, or complained about him, anything that might be helpful,\" said Couvertier. Wiles\\' parents told CNN they hope their son Robert is still alive. \"That is our hope until proven otherwise, \\' said his mother, Pamela. \"We don\\'t know what happened to him. We don\\'t know where he is. We just hope somebody will call and tell us what their needs are,\" she added. \"Sometimes, I stay up late and get up early and try to come up with any scenarios to make sense of this,\" said Wiles\\' father. \"I have my very low moments when I think I\\'ll never see him again, \" he added. Occasionally, Wiles\\' mother says she plays back a voicemail message her son left her shortly before he disappeared. It said \"Hey, mom. It\\'s Robert. I just wanted to call and thank you for sending that stuff down to me. And I hope you have a good weekend, ... and I\\'ll talk to you later.\" \"I still listen to it, and it breaks my heart,\" said Pamela Wiles. The Wiles are still offering a $250,000 reward for information about their son and arrest of those responsible for his disappearance. \"But Tom and I don\\'t think you can put a dollar on his head,\" said Pamela Wiles. \"We want to provide them with security and a new life, if that\\'s what they need,\" she added. His father doesn\\'t think whoever is responsible is working alone. He says his son is too strong to have left willingly or without knowing who his alleged captors are. Anyone with information is asked to contact the FBI. Wiles\\' parents have a message for the FBI\\'s unidentified suspect. \"I would say to him \\'come forward and tell the truth and we hold no personal animosity,\\'\" said Pamela Wiles. \"Someone made a bad mistake. It\\'s up to God to forgive him.\"',\n",
       " 'highlights': 'FBI agent says they are \"close to solving the case,\" have a \"key suspect\"\\n\"Key suspect\" is in Florida, either in Orlando, Lakeland or Melbourne, agent says .\\nPolice believe missing pilot Robert Wiles may have been kidnapped for ransom .\\nMother would tell suspect to \"come forward and tell the truth\"',\n",
       " 'id': '772858186d456cbd32232325ba43db6ad669870c'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examining the first record of the dataset.  \n",
    "cnn_news_summary_ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2ba90-08bf-4d8b-830d-35a045b1d2cf",
   "metadata": {},
   "source": [
    "## Preprocessing and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8588afe-2041-40de-81e0-7ad0001b63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a text cleaning function.  This function iterates over the 'article'\n",
    "# and 'highlights' section and replaces various text strings (like\n",
    "#backslashes, new lines, etc.) with the empty string\n",
    "\n",
    "def clean_txt(example):\n",
    "  for txt in ['article', 'highlights']:\n",
    "    example[txt] = example[txt].lower() #convert text to lowercase\n",
    "    example[txt] = example[txt].replace('\\\\','')\n",
    "    example[txt] = example[txt].replace('/','')\n",
    "    example[txt] = example[txt].replace('\\n','')\n",
    "    example[txt] = example[txt].replace('``','')\n",
    "    example[txt] = example[txt].replace('\"','')\n",
    "    example[txt] = example[txt].replace('--','')\n",
    "  return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba7aa7-ccdb-44f4-a3ef-3ab7a9d7848a",
   "metadata": {},
   "source": [
    "### Mapping the text cleaning function to the training dataset and the holdout dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf32d74-311a-4f66-89a1-42bd1c655333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2783ee1dc19446938d689c2f9562d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f719d531a28847c2a21ae54385044fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 2296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hugging face datasets allow the .map operation to\n",
    "#apply a function to all records in a dataset, and then will\n",
    "#update the dataset.  In efect, the .map() method\n",
    "# maps the `clean_txt` function to\n",
    "# all the records in the `cnn_news_summary_ds dataset.\n",
    "#The result will be that the training and\n",
    "#test data will now be cleaned\n",
    "cleaned_cnn_news_summary_ds = cnn_news_summary_ds.map(clean_txt)\n",
    "\n",
    "cleaned_cnn_news_summary_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ad4f20-ab18-4fac-a51f-3225741a1230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== Original dataset ====\n",
      "\n",
      "\n",
      "(CNN) -- After almost 10 months, the FBI has zeroed in on a suspect in the case of missing Florida pilot Robert Wiles, who may have been kidnapped for ransom. Missing Florida pilot Robert Wiles is thought to have been kidnapped for ransom. \"We're close to solving the case,\" said FBI special agent David Couvertier. He would not elaborate. Agents also would not identify the suspect, and they said the person is not in custody. Investigators would only reveal that the \"key suspect\" is in Florida, either in Orlando, Lakeland or Melbourne. \"They're holding that back in hopes of getting additional information,\" said Couvertier. The FBI says it's also looking at several persons of interest in those same three Florida cities. Wiles, 27, was last seen in the family's aircraft maintenance business, National Flight Services, at Lakeland Linder Regional Airport on April 1, 2008. The day Wiles disappeared, he left behind his bags, his computer, and even his car. His father says the next day, Wiles was supposed to be on a flight out of Orlando. He never showed up. Two days after he vanished, Wiles' father, Thomas, received a ransom note. It demanded money and threatened to harm Robert Wiles if the terms weren't met. Wiles' parents said they tried to comply but heard nothing back. Nearly a year later, the FBI says their investigation shows that those involved \"were very familiar with Robert's work, Robert's personal information and had knowledge of his parent's personal affairs.\" Agents say they've tracked down leads in nine U.S. cities and as far away as Thailand where National Flight Services also does business. The Ohio-based company services aircraft in 50 countries and has been in business since 1972. Investigators are reaching out to the public in hopes of sparking any additional information about Wiles to \"close the loop,\" Couvertier said. Investigators want to talk with \"people who knew about his work routine, who might know someone who was obsessed with him, who was upset or jealous of him, or complained about him, anything that might be helpful,\" said Couvertier. Wiles' parents told CNN they hope their son Robert is still alive. \"That is our hope until proven otherwise, ' said his mother, Pamela. \"We don't know what happened to him. We don't know where he is. We just hope somebody will call and tell us what their needs are,\" she added. \"Sometimes, I stay up late and get up early and try to come up with any scenarios to make sense of this,\" said Wiles' father. \"I have my very low moments when I think I'll never see him again, \" he added. Occasionally, Wiles' mother says she plays back a voicemail message her son left her shortly before he disappeared. It said \"Hey, mom. It's Robert. I just wanted to call and thank you for sending that stuff down to me. And I hope you have a good weekend, ... and I'll talk to you later.\" \"I still listen to it, and it breaks my heart,\" said Pamela Wiles. The Wiles are still offering a $250,000 reward for information about their son and arrest of those responsible for his disappearance. \"But Tom and I don't think you can put a dollar on his head,\" said Pamela Wiles. \"We want to provide them with security and a new life, if that's what they need,\" she added. His father doesn't think whoever is responsible is working alone. He says his son is too strong to have left willingly or without knowing who his alleged captors are. Anyone with information is asked to contact the FBI. Wiles' parents have a message for the FBI's unidentified suspect. \"I would say to him 'come forward and tell the truth and we hold no personal animosity,'\" said Pamela Wiles. \"Someone made a bad mistake. It's up to God to forgive him.\"\n",
      "\n",
      "\n",
      "==== Cleaned dataset ====\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(cnn)  after almost 10 months, the fbi has zeroed in on a suspect in the case of missing florida pilot robert wiles, who may have been kidnapped for ransom. missing florida pilot robert wiles is thought to have been kidnapped for ransom. we're close to solving the case, said fbi special agent david couvertier. he would not elaborate. agents also would not identify the suspect, and they said the person is not in custody. investigators would only reveal that the key suspect is in florida, either in orlando, lakeland or melbourne. they're holding that back in hopes of getting additional information, said couvertier. the fbi says it's also looking at several persons of interest in those same three florida cities. wiles, 27, was last seen in the family's aircraft maintenance business, national flight services, at lakeland linder regional airport on april 1, 2008. the day wiles disappeared, he left behind his bags, his computer, and even his car. his father says the next day, wiles was supposed to be on a flight out of orlando. he never showed up. two days after he vanished, wiles' father, thomas, received a ransom note. it demanded money and threatened to harm robert wiles if the terms weren't met. wiles' parents said they tried to comply but heard nothing back. nearly a year later, the fbi says their investigation shows that those involved were very familiar with robert's work, robert's personal information and had knowledge of his parent's personal affairs. agents say they've tracked down leads in nine u.s. cities and as far away as thailand where national flight services also does business. the ohio-based company services aircraft in 50 countries and has been in business since 1972. investigators are reaching out to the public in hopes of sparking any additional information about wiles to close the loop, couvertier said. investigators want to talk with people who knew about his work routine, who might know someone who was obsessed with him, who was upset or jealous of him, or complained about him, anything that might be helpful, said couvertier. wiles' parents told cnn they hope their son robert is still alive. that is our hope until proven otherwise, ' said his mother, pamela. we don't know what happened to him. we don't know where he is. we just hope somebody will call and tell us what their needs are, she added. sometimes, i stay up late and get up early and try to come up with any scenarios to make sense of this, said wiles' father. i have my very low moments when i think i'll never see him again,  he added. occasionally, wiles' mother says she plays back a voicemail message her son left her shortly before he disappeared. it said hey, mom. it's robert. i just wanted to call and thank you for sending that stuff down to me. and i hope you have a good weekend, ... and i'll talk to you later. i still listen to it, and it breaks my heart, said pamela wiles. the wiles are still offering a $250,000 reward for information about their son and arrest of those responsible for his disappearance. but tom and i don't think you can put a dollar on his head, said pamela wiles. we want to provide them with security and a new life, if that's what they need, she added. his father doesn't think whoever is responsible is working alone. he says his son is too strong to have left willingly or without knowing who his alleged captors are. anyone with information is asked to contact the fbi. wiles' parents have a message for the fbi's unidentified suspect. i would say to him 'come forward and tell the truth and we hold no personal animosity,' said pamela wiles. someone made a bad mistake. it's up to god to forgive him.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying that we have a clean training dataset by comparing a record from the original\n",
    "#dataset to the cleaned dataset\n",
    "print('\\n\\n==== Original dataset ====\\n\\n')\n",
    "\n",
    "print (cnn_news_summary_ds['train']['article'][0])\n",
    "\n",
    "print('\\n\\n==== Cleaned dataset ====\\n\\n')\n",
    "\n",
    "cleaned_cnn_news_summary_ds['train']['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5934ee42-7bc3-4709-a97a-4a69822aad2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09acedb546249aa885c71adf99b34fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_cnn_holdout_ds = cnn_holdout_ds.map(clean_txt)\n",
    "\n",
    "cleaned_cnn_holdout_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de75e6f-54f5-466b-8594-b70e01bbc053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== Original holdout dataset ====\n",
      "\n",
      "\n",
      "Manchester United have fallen off their perch. And they’re dropping like a stone towards mediocrity. That is the undeniable fact that has been hammered home relentlessly during the past six months. Whether we are talking about the events of Wednesday night at Olympiacos or before the startled eyes of the faithful at Old Trafford, the evidence is there for all to see. Can't stop the slump: David Moyes cannot believe it as he watches Manchester United lose at Olympiacos . Down and almost out: Robin van Persie lies on the floor during a defeat which sees United's Champions League campaign hanging by a thread . Disbelief: Wayne Rooney cries out in vain during another shambolic United display . Abject: The frustration shows on the Man United players' faces on taking the restart after conceding to Olympiacos . Coming to get you: Liverpool are looking to take United's place in the top four . Now is it time for Man United to sack Moyes? Out of the title race, out of the FA . Cup, out of the League Cup, out of the top four and now in desperate . need of an improbable recovery in the Champions League. On what we have all seen so far this season, who would bet upon them turning the tables? The Manchester United of seasons past, maybe. The Manchester United of Sir Alex Ferguson. The . snarling face of Roy Keane, the passion of Gary Neville, the . understated brilliance of Paul Scholes, the delivery of David Beckham. And behind it all, the aggression and defiance of Peter Schmeichel. Only those characters are no more. David Moyes is being haunted by those ghosts of the past. There is no snarling, there is no brilliance, there is certainly no delivery. Win at all costs? No, what passed before Moyes’s eyes in the Greek port city was his worst nightmare. There was more passion shown on the sidelines by Roy Keane on television than there was on the pitch. (Note . to Lisa Carrick: pick a fight that you can win. Keane was right. It was . an insipid, pathetic excuse of an interview that mirrored the . performance.) Scratching his head: Moyes rues the defeat by Olympiacos and wonders what to do next . Nowhere to hide: Rooney (left), Michael Carrick (centre) and Rio Ferdinand feel the pain of defeat . Pointless: Moyes takes out his frustration at the humiliating defeat on a match official after the match . Triumph: The Olympiacos players enjoy their famous victory over Man Utd as Danny Welbeck (left) looks on . If we start with the manager, we have to ask: has he become utterly incapable overnight? Of course not. Here . was a man who overachieved at Everton, understanding what it took to keep the club fighting it out with the big boys in the top half. But the one aspect of the managerial job that Moyes does not yet possess at Old Trafford is the fear factor. How many times was Ferguson prepared to stand toe-to-toe in the dressing room and slug it out with his players? That . fear factor, the fear of losing your place, the fear of not playing for . one of the world’s biggest clubs, the fear of being told you are not . wanted.... It looks like it doesn’t matter to half of them. Do you know what? It doesn’t any more. Protected by grotesque contracts, the lack of desire was startling. The only two who can escape criticism from Wednesday night’s display were Nemanja Vidic and Wayne Rooney. One has just signed a mammoth new pay deal. The other one is off at the end of the season. I thought Vidic was magnificent. Pointed failure: Stoke City's Charlie Adam celebrates scoring in the win over Man Utd in the Premier League . Cup shock: Swansea's Wilfried Bony (second left) celebrates knocking Man Utd out of the FA Cup at Old Trafford . Mixed emotions: Sunderland go wild after their Capital One Cup penalty win, while Adnan Januzaj suffers . But the remainder of it is sub-standard. There is an ageing spine to the team. Rio Ferdinand. Gone. Chris Smalling. Not good enough. Ditto the entire back four. And Tom Cleverley. Not Manchester United standard. You can include Ashley Young and Antonio Valencia in that bracket, too. While I can sit here and say that they aren’t good enough, that doesn’t excuse the lack of desire. Where was it? Where was that team of winners? Let’s . face it, Moyes may have inherited a poor situation, not helped by the . fact that David Gill, a steady hand on the tiller, departed at the same . time as Ferguson. But he could at least instil some pride in the shirt. This is, though, a situation that has been coming. The writing has been on the wall for a while. Yes, people point to the title race last season. But let’s look at the state United’s rivals were in. Chelsea . suffered a hangover from the Champions League. Their own end of an era. Arsenal weren’t anywhere near as strong as they are this year. And they . look like they will fall short - again. And it appears there was a player backlash against Roberto Mancini that did for him at Manchester CIty. Outspoken: Van Persie has suggested his team-mates are affecting his form by taking up his positions . On his way: Nemanja Vidic (left) is set to join Inter Milan imminently . Getting the boot: It's looking like  Ferdinand's final season at Old Trafford . Sub-standard: Tom Cleverley is not good enough for United, and neither is Ashley Young (right) How . would Manchester United’s class of 2012-13 get on this season? Hmm. Better, but not winning the crown by a country mile, that’s for sure. The . true state of the club was laid bare when Athletic Bilbao outclassed . them in the Europa League a couple of seasons ago. That really should . have set the alarm bells ringing. And how many players - apart from the front two - would other top-five Premier League teams take? That’s the level we’re talking about. And Chelsea have just off-loaded one they deem surplus to United. What’s . worse is that there isn’t anyone else any better to come in. Sergio . Aguero/David Silva isn’t standing injured on the sidelines. There are untold reasons United are struggling. But . the fact of the matter is the players aren’t good enough. The past . manager may or may not have known it. The current one most certainly . does. The truth laid bare: The real state of United was revealed when they lost to Athletic Bilbao in the Europa League in 2012 . Oscar winner: Oscar De Marcos celebrates scoring for Athletic against United in that tie . What is most chilling . is that this situation mirrors the one that Graeme Souness found himself . in when he took over at Liverpool 23 years ago. A golden generation had passed. The Scot had to rip it up and start again. And the fall-out was painful. The . rest of football now scents blood. The way they did when Anfield lost . its aura. That’s why teams such as West Brom, Swansea and Everton have . emerged from Old Trafford with what has been, until this season, . once-in-a-lifetime victories. Souness never really managed to find the right cocktail to replace a generation of giants. Moyes finds himself lagging behind Arsenal, Manchester City, Chelsea and, yes, Liverpool. On . a European front, Bayern Munich, PSG, Monaco and Borussia Dortmund and . the two Spanish giants would not fear this current United set-up. True leader: United are missing a great figurehead such as Roy Keane, whose snarling face, such a feature of successful past Old Trafford teams, is now only seen on ITV (below) passing judgement on current failings . Moyes needs giants. Giant characters. Giant players. It’s going to cost. But what’s the alternative? More mediocrity? Manchester United need to sign some top-quality players. Because, at the moment, apart from in one or two cases, they’re not of the quality the champions have come to expect.\n",
      "\n",
      "\n",
      "==== Cleaned holdout dataset ====\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"manchester united have fallen off their perch. and they’re dropping like a stone towards mediocrity. that is the undeniable fact that has been hammered home relentlessly during the past six months. whether we are talking about the events of wednesday night at olympiacos or before the startled eyes of the faithful at old trafford, the evidence is there for all to see. can't stop the slump: david moyes cannot believe it as he watches manchester united lose at olympiacos . down and almost out: robin van persie lies on the floor during a defeat which sees united's champions league campaign hanging by a thread . disbelief: wayne rooney cries out in vain during another shambolic united display . abject: the frustration shows on the man united players' faces on taking the restart after conceding to olympiacos . coming to get you: liverpool are looking to take united's place in the top four . now is it time for man united to sack moyes? out of the title race, out of the fa . cup, out of the league cup, out of the top four and now in desperate . need of an improbable recovery in the champions league. on what we have all seen so far this season, who would bet upon them turning the tables? the manchester united of seasons past, maybe. the manchester united of sir alex ferguson. the . snarling face of roy keane, the passion of gary neville, the . understated brilliance of paul scholes, the delivery of david beckham. and behind it all, the aggression and defiance of peter schmeichel. only those characters are no more. david moyes is being haunted by those ghosts of the past. there is no snarling, there is no brilliance, there is certainly no delivery. win at all costs? no, what passed before moyes’s eyes in the greek port city was his worst nightmare. there was more passion shown on the sidelines by roy keane on television than there was on the pitch. (note . to lisa carrick: pick a fight that you can win. keane was right. it was . an insipid, pathetic excuse of an interview that mirrored the . performance.) scratching his head: moyes rues the defeat by olympiacos and wonders what to do next . nowhere to hide: rooney (left), michael carrick (centre) and rio ferdinand feel the pain of defeat . pointless: moyes takes out his frustration at the humiliating defeat on a match official after the match . triumph: the olympiacos players enjoy their famous victory over man utd as danny welbeck (left) looks on . if we start with the manager, we have to ask: has he become utterly incapable overnight? of course not. here . was a man who overachieved at everton, understanding what it took to keep the club fighting it out with the big boys in the top half. but the one aspect of the managerial job that moyes does not yet possess at old trafford is the fear factor. how many times was ferguson prepared to stand toe-to-toe in the dressing room and slug it out with his players? that . fear factor, the fear of losing your place, the fear of not playing for . one of the world’s biggest clubs, the fear of being told you are not . wanted.... it looks like it doesn’t matter to half of them. do you know what? it doesn’t any more. protected by grotesque contracts, the lack of desire was startling. the only two who can escape criticism from wednesday night’s display were nemanja vidic and wayne rooney. one has just signed a mammoth new pay deal. the other one is off at the end of the season. i thought vidic was magnificent. pointed failure: stoke city's charlie adam celebrates scoring in the win over man utd in the premier league . cup shock: swansea's wilfried bony (second left) celebrates knocking man utd out of the fa cup at old trafford . mixed emotions: sunderland go wild after their capital one cup penalty win, while adnan januzaj suffers . but the remainder of it is sub-standard. there is an ageing spine to the team. rio ferdinand. gone. chris smalling. not good enough. ditto the entire back four. and tom cleverley. not manchester united standard. you can include ashley young and antonio valencia in that bracket, too. while i can sit here and say that they aren’t good enough, that doesn’t excuse the lack of desire. where was it? where was that team of winners? let’s . face it, moyes may have inherited a poor situation, not helped by the . fact that david gill, a steady hand on the tiller, departed at the same . time as ferguson. but he could at least instil some pride in the shirt. this is, though, a situation that has been coming. the writing has been on the wall for a while. yes, people point to the title race last season. but let’s look at the state united’s rivals were in. chelsea . suffered a hangover from the champions league. their own end of an era. arsenal weren’t anywhere near as strong as they are this year. and they . look like they will fall short - again. and it appears there was a player backlash against roberto mancini that did for him at manchester city. outspoken: van persie has suggested his team-mates are affecting his form by taking up his positions . on his way: nemanja vidic (left) is set to join inter milan imminently . getting the boot: it's looking like  ferdinand's final season at old trafford . sub-standard: tom cleverley is not good enough for united, and neither is ashley young (right) how . would manchester united’s class of 2012-13 get on this season? hmm. better, but not winning the crown by a country mile, that’s for sure. the . true state of the club was laid bare when athletic bilbao outclassed . them in the europa league a couple of seasons ago. that really should . have set the alarm bells ringing. and how many players - apart from the front two - would other top-five premier league teams take? that’s the level we’re talking about. and chelsea have just off-loaded one they deem surplus to united. what’s . worse is that there isn’t anyone else any better to come in. sergio . aguerodavid silva isn’t standing injured on the sidelines. there are untold reasons united are struggling. but . the fact of the matter is the players aren’t good enough. the past . manager may or may not have known it. the current one most certainly . does. the truth laid bare: the real state of united was revealed when they lost to athletic bilbao in the europa league in 2012 . oscar winner: oscar de marcos celebrates scoring for athletic against united in that tie . what is most chilling . is that this situation mirrors the one that graeme souness found himself . in when he took over at liverpool 23 years ago. a golden generation had passed. the scot had to rip it up and start again. and the fall-out was painful. the . rest of football now scents blood. the way they did when anfield lost . its aura. that’s why teams such as west brom, swansea and everton have . emerged from old trafford with what has been, until this season, . once-in-a-lifetime victories. souness never really managed to find the right cocktail to replace a generation of giants. moyes finds himself lagging behind arsenal, manchester city, chelsea and, yes, liverpool. on . a european front, bayern munich, psg, monaco and borussia dortmund and . the two spanish giants would not fear this current united set-up. true leader: united are missing a great figurehead such as roy keane, whose snarling face, such a feature of successful past old trafford teams, is now only seen on itv (below) passing judgement on current failings . moyes needs giants. giant characters. giant players. it’s going to cost. but what’s the alternative? more mediocrity? manchester united need to sign some top-quality players. because, at the moment, apart from in one or two cases, they’re not of the quality the champions have come to expect.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying that we have a clean holdout dataset by comparing a record from the original\n",
    "#holdout dataset to the cleaned holdout dataset\n",
    "print('\\n\\n==== Original holdout dataset ====\\n\\n')\n",
    "\n",
    "print (cnn_holdout_ds['article'][0])\n",
    "\n",
    "print('\\n\\n==== Cleaned holdout dataset ====\\n\\n')\n",
    "\n",
    "cleaned_cnn_holdout_ds['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e85d5089-515e-4442-8c62-3e88693a0a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9045d15a64509896937c3b59d88e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "922232"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_cnn_holdout_ds.to_csv('data/cleaned_cnn_holdout_ds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8ec89df-5dad-441c-accd-d351e713dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=256, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0140e-8b60-4c0a-9a9f-a46c14ddb340",
   "metadata": {},
   "source": [
    "## Instantiating an \"off-the-shelf\" t5 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec1e004-e12e-4914-8cc3-209e5acb2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5ForConditionalGeneration, TrainingArguments, Trainer, \\\n",
    "                         DataCollatorForSeq2Seq, T5Tokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aa39820-cc67-4796-bc62-9f1e97e6fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c4f1f54-5033-44aa-86ac-4b2c756a553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at model architecture\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716c456-d74d-44ae-8b89-b38d9df879c3",
   "metadata": {},
   "source": [
    "## Preprocessing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7192884d-6529-4014-8dac-1c20b31773fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a preprocessing function\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=256, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff8fff-a13e-43a8-af71-24bab23d09c1",
   "metadata": {},
   "source": [
    "### Preprocessing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52dab96-2aad-46d6-8855-2e4576eb0946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6c8447f8ce4e0cb3c5b0de3ee9b177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2296 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee26a6a8484b29a4cf66aa9285009a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2296\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_cnn_training_ds = cleaned_cnn_news_summary_ds.map(preprocess_function, batched=True)\n",
    "tokenized_cnn_training_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fe8a4-ce72-41f8-9066-08705441a955",
   "metadata": {},
   "source": [
    "### Preprocessing the holdout dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c9ae584-1352-4cf5-9b2c-6e1bb9235636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb16f4ce45641da9ce956af6ec2a9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_cnn_holdout_ds = cleaned_cnn_holdout_ds.map(preprocess_function, batched=True)\n",
    "tokenized_cnn_holdout_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c555d67a-c58b-4a96-b036-357bbd201ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fcfe811bf445a993612a7bd7d55593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1563405"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized_cnn_holdout_ds.to_csv('data/tokenized_cnn_holdout_ds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14641065-95cd-42c7-8c76-b8b3e7c046fa",
   "metadata": {},
   "source": [
    "## Instantiating a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c0d562-5b47-486f-a0f3-0d954dd72cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acee71d-e5a1-4f4f-a9a2-58ed3ae8b46d",
   "metadata": {},
   "source": [
    "## Importing ROUGE Metric\n",
    "*  Rouge is a standard evaluation metric used in text summarization tasks\n",
    "*  ROUGE provides an *objective metric* to compare model-produced summary with the dataset's reference summary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee15cc-38f5-41f7-8221-4f29f853e9a6",
   "metadata": {},
   "source": [
    "### Importing the evaluate library\n",
    "\n",
    "The 'evaluate' library from Hugging Face allows us to evaluate ML models.  The 'evaluate' library provides access to dozens of evaluation metrics across many ML domains (including NLP, computer vision, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93f8e1a-a577-4ff6-9ec8-e2d66dbf4538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (f1),\n",
       "    rouge2: rouge_2 (f1),\n",
       "    rougeL: rouge_l (f1),\n",
       "    rougeLsum: rouge_lsum (f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = evaluate.load('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "148bb568-8435-4a15-8b84-51a9ecbc2334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.37163589198592073,\n",
       " 'rouge2': 0.1659438181778043,\n",
       " 'rougeL': 0.26232053780080417,\n",
       " 'rougeLsum': 0.2625393886964161}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregated results of inference on holdout set\n",
    "result_off_the_shelf_agg = rouge.compute(predictions = holdout_off_the_shelf_summaries,\n",
    "                           references = holdout_article_summaries[:],\n",
    "                           use_stemmer=True)\n",
    "\n",
    "result_off_the_shelf_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a04863bf-e067-49bf-99fb-c00021ac8071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': [0.2839506172839506,\n",
       "  0.45714285714285713,\n",
       "  0.4054054054054054,\n",
       "  0.352112676056338,\n",
       "  0.3859649122807018,\n",
       "  0.2,\n",
       "  0.5393258426966292,\n",
       "  0.3333333333333333,\n",
       "  0.3684210526315789,\n",
       "  0.2823529411764706,\n",
       "  0.41666666666666663,\n",
       "  0.46031746031746035,\n",
       "  0.40449438202247195,\n",
       "  0.3392857142857143,\n",
       "  0.5057471264367817,\n",
       "  0.29729729729729726,\n",
       "  0.25396825396825395,\n",
       "  0.35897435897435903,\n",
       "  0.6233766233766234,\n",
       "  0.43243243243243246,\n",
       "  0.42105263157894735,\n",
       "  0.6923076923076923,\n",
       "  0.28571428571428575,\n",
       "  0.4155844155844156,\n",
       "  0.15384615384615385,\n",
       "  0.24242424242424243,\n",
       "  0.3137254901960784,\n",
       "  0.33043478260869563,\n",
       "  0.3555555555555555,\n",
       "  0.5569620253164557,\n",
       "  0.34285714285714286,\n",
       "  0.32,\n",
       "  0.3826086956521739,\n",
       "  0.3103448275862069,\n",
       "  0.5185185185185185,\n",
       "  0.2975206611570248,\n",
       "  0.4000000000000001,\n",
       "  0.43809523809523804,\n",
       "  0.4225352112676056,\n",
       "  0.3636363636363636,\n",
       "  0.4639999999999999,\n",
       "  0.54,\n",
       "  0.4878048780487805,\n",
       "  0.31111111111111117,\n",
       "  0.3157894736842105,\n",
       "  0.3013698630136986,\n",
       "  0.32098765432098764,\n",
       "  0.2564102564102564,\n",
       "  0.684931506849315,\n",
       "  0.4,\n",
       "  0.4090909090909091,\n",
       "  0.17821782178217818,\n",
       "  1.0,\n",
       "  0.22916666666666669,\n",
       "  0.4415584415584416,\n",
       "  0.22330097087378642,\n",
       "  0.17073170731707316,\n",
       "  0.44660194174757284,\n",
       "  0.20634920634920634,\n",
       "  0.3026315789473684,\n",
       "  0.25316455696202533,\n",
       "  0.27083333333333337,\n",
       "  0.3496503496503497,\n",
       "  0.4057971014492754,\n",
       "  0.2268041237113402,\n",
       "  0.4485981308411215,\n",
       "  0.43859649122807015,\n",
       "  0.6713286713286714,\n",
       "  0.1951219512195122,\n",
       "  0.4036697247706422,\n",
       "  0.4090909090909091,\n",
       "  0.11464968152866244,\n",
       "  0.20289855072463767,\n",
       "  0.4307692307692308,\n",
       "  0.45,\n",
       "  0.5684210526315789,\n",
       "  0.4578313253012048,\n",
       "  0.3823529411764706,\n",
       "  0.18421052631578944,\n",
       "  0.28571428571428575,\n",
       "  0.2150537634408602,\n",
       "  0.40384615384615385,\n",
       "  0.41420118343195267,\n",
       "  0.5205479452054794,\n",
       "  0.33333333333333337,\n",
       "  0.47457627118644063,\n",
       "  0.6666666666666666,\n",
       "  0.37500000000000006,\n",
       "  0.29629629629629634,\n",
       "  0.2247191011235955,\n",
       "  0.4000000000000001,\n",
       "  0.5043478260869565,\n",
       "  0.5789473684210527,\n",
       "  0.35789473684210527,\n",
       "  0.3,\n",
       "  0.24561403508771934,\n",
       "  0.5165562913907286,\n",
       "  0.6666666666666666,\n",
       "  0.46,\n",
       "  0.32608695652173914,\n",
       "  0.40816326530612246,\n",
       "  0.1935483870967742,\n",
       "  0.30588235294117644,\n",
       "  0.48648648648648646,\n",
       "  0.3658536585365854,\n",
       "  0.3888888888888889,\n",
       "  0.2481751824817518,\n",
       "  0.5070422535211268,\n",
       "  0.39285714285714285,\n",
       "  0.22727272727272727,\n",
       "  0.3058823529411765,\n",
       "  0.4273504273504274,\n",
       "  0.411214953271028,\n",
       "  0.31460674157303375,\n",
       "  0.4050632911392405,\n",
       "  0.4444444444444445,\n",
       "  0.43697478991596633,\n",
       "  0.3013698630136986,\n",
       "  0.3448275862068966,\n",
       "  0.44800000000000006,\n",
       "  0.18666666666666668,\n",
       "  0.5473684210526316,\n",
       "  0.36000000000000004,\n",
       "  0.40601503759398494,\n",
       "  0.43478260869565216,\n",
       "  0.34375000000000006,\n",
       "  0.34146341463414637,\n",
       "  0.4827586206896552,\n",
       "  0.3050847457627119,\n",
       "  0.30769230769230765,\n",
       "  0.15053763440860213,\n",
       "  0.2222222222222222,\n",
       "  0.20930232558139536,\n",
       "  0.375,\n",
       "  0.2580645161290322,\n",
       "  0.43243243243243246,\n",
       "  0.36111111111111116,\n",
       "  0.5882352941176471,\n",
       "  0.4,\n",
       "  0.4186046511627907,\n",
       "  0.2531645569620253,\n",
       "  0.2941176470588235,\n",
       "  0.30612244897959184,\n",
       "  0.39999999999999997,\n",
       "  0.2857142857142857,\n",
       "  0.5714285714285714,\n",
       "  0.08333333333333333,\n",
       "  0.3953488372093023,\n",
       "  0.5911949685534591,\n",
       "  0.4084507042253521,\n",
       "  0.35714285714285715,\n",
       "  0.16666666666666666,\n",
       "  0.4175824175824176,\n",
       "  0.3508771929824562,\n",
       "  0.1694915254237288,\n",
       "  0.4077669902912621,\n",
       "  0.16901408450704225,\n",
       "  0.37735849056603776,\n",
       "  0.2553191489361702,\n",
       "  0.08571428571428572,\n",
       "  0.4516129032258065,\n",
       "  0.358974358974359,\n",
       "  0.28125000000000006,\n",
       "  0.6046511627906976,\n",
       "  0.2857142857142857,\n",
       "  0.39999999999999997,\n",
       "  0.39669421487603307,\n",
       "  0.044444444444444446,\n",
       "  0.4195804195804196,\n",
       "  0.6732673267326733,\n",
       "  0.19999999999999998,\n",
       "  0.3934426229508197,\n",
       "  0.41666666666666674,\n",
       "  0.4,\n",
       "  0.345679012345679,\n",
       "  0.4666666666666666,\n",
       "  0.3577235772357723,\n",
       "  0.41758241758241765,\n",
       "  0.35658914728682173,\n",
       "  0.4742268041237113,\n",
       "  0.403225806451613,\n",
       "  0.37037037037037035,\n",
       "  0.4696969696969697,\n",
       "  0.4065040650406504,\n",
       "  0.21428571428571433,\n",
       "  0.5242718446601942,\n",
       "  0.32727272727272727,\n",
       "  0.47887323943661975,\n",
       "  0.16326530612244897,\n",
       "  0.2553191489361702,\n",
       "  0.3809523809523809,\n",
       "  0.2696629213483146,\n",
       "  0.18666666666666668,\n",
       "  0.5904761904761904,\n",
       "  0.4210526315789473,\n",
       "  0.39694656488549623,\n",
       "  0.5185185185185185,\n",
       "  0.2945736434108527,\n",
       "  0.2325581395348837,\n",
       "  0.41758241758241754],\n",
       " 'rouge2': [0.1,\n",
       "  0.2753623188405797,\n",
       "  0.1111111111111111,\n",
       "  0.08571428571428572,\n",
       "  0.14545454545454545,\n",
       "  0.06896551724137931,\n",
       "  0.32183908045977017,\n",
       "  0.11428571428571428,\n",
       "  0.2702702702702703,\n",
       "  0.04819277108433735,\n",
       "  0.1702127659574468,\n",
       "  0.3709677419354839,\n",
       "  0.13793103448275862,\n",
       "  0.07272727272727272,\n",
       "  0.32941176470588235,\n",
       "  0.05555555555555555,\n",
       "  0.03278688524590164,\n",
       "  0.1391304347826087,\n",
       "  0.5066666666666667,\n",
       "  0.11009174311926606,\n",
       "  0.2150537634408602,\n",
       "  0.5526315789473685,\n",
       "  0.08823529411764706,\n",
       "  0.16,\n",
       "  0.039215686274509796,\n",
       "  0.125,\n",
       "  0.11920529801324506,\n",
       "  0.23008849557522124,\n",
       "  0.11363636363636365,\n",
       "  0.33766233766233766,\n",
       "  0.07766990291262137,\n",
       "  0.10958904109589042,\n",
       "  0.10619469026548672,\n",
       "  0.03571428571428571,\n",
       "  0.17721518987341772,\n",
       "  0.05042016806722689,\n",
       "  0.1238938053097345,\n",
       "  0.15533980582524273,\n",
       "  0.20289855072463767,\n",
       "  0.186046511627907,\n",
       "  0.3252032520325203,\n",
       "  0.5102040816326531,\n",
       "  0.225,\n",
       "  0.13636363636363635,\n",
       "  0.08928571428571429,\n",
       "  0.11267605633802817,\n",
       "  0.0759493670886076,\n",
       "  0.05405405405405406,\n",
       "  0.47887323943661964,\n",
       "  0.2040816326530612,\n",
       "  0.18604651162790697,\n",
       "  0.04040404040404041,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.21333333333333332,\n",
       "  0.08823529411764706,\n",
       "  0.0,\n",
       "  0.19801980198019803,\n",
       "  0.03225806451612904,\n",
       "  0.17333333333333334,\n",
       "  0.0779220779220779,\n",
       "  0.0851063829787234,\n",
       "  0.07092198581560283,\n",
       "  0.23880597014925373,\n",
       "  0.08421052631578949,\n",
       "  0.26666666666666666,\n",
       "  0.21428571428571427,\n",
       "  0.6382978723404257,\n",
       "  0.024999999999999998,\n",
       "  0.1869158878504673,\n",
       "  0.25581395348837205,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1875,\n",
       "  0.28205128205128205,\n",
       "  0.3870967741935484,\n",
       "  0.22222222222222227,\n",
       "  0.12121212121212123,\n",
       "  0.0,\n",
       "  0.0625,\n",
       "  0.13186813186813184,\n",
       "  0.25490196078431376,\n",
       "  0.26347305389221554,\n",
       "  0.36619718309859156,\n",
       "  0.025974025974025972,\n",
       "  0.1724137931034483,\n",
       "  0.65625,\n",
       "  0.1282051282051282,\n",
       "  0.10126582278481013,\n",
       "  0.06896551724137931,\n",
       "  0.14457831325301204,\n",
       "  0.2831858407079646,\n",
       "  0.29729729729729726,\n",
       "  0.1935483870967742,\n",
       "  0.1836734693877551,\n",
       "  0.03636363636363636,\n",
       "  0.21476510067114093,\n",
       "  0.5753424657534247,\n",
       "  0.1836734693877551,\n",
       "  0.06666666666666667,\n",
       "  0.1793103448275862,\n",
       "  0.04395604395604396,\n",
       "  0.12048192771084337,\n",
       "  0.3424657534246575,\n",
       "  0.125,\n",
       "  0.22641509433962265,\n",
       "  0.0,\n",
       "  0.2898550724637681,\n",
       "  0.21818181818181817,\n",
       "  0.046511627906976744,\n",
       "  0.0963855421686747,\n",
       "  0.34782608695652173,\n",
       "  0.20952380952380953,\n",
       "  0.04597701149425287,\n",
       "  0.20512820512820512,\n",
       "  0.16393442622950818,\n",
       "  0.23931623931623933,\n",
       "  0.028169014084507043,\n",
       "  0.07058823529411765,\n",
       "  0.16260162601626016,\n",
       "  0.0547945205479452,\n",
       "  0.21505376344086025,\n",
       "  0.163265306122449,\n",
       "  0.12213740458015267,\n",
       "  0.22222222222222224,\n",
       "  0.03225806451612903,\n",
       "  0.07499999999999998,\n",
       "  0.2857142857142857,\n",
       "  0.08620689655172413,\n",
       "  0.0784313725490196,\n",
       "  0.0,\n",
       "  0.038461538461538464,\n",
       "  0.04761904761904762,\n",
       "  0.09090909090909093,\n",
       "  0.03333333333333333,\n",
       "  0.16666666666666669,\n",
       "  0.11267605633802817,\n",
       "  0.36363636363636365,\n",
       "  0.16666666666666666,\n",
       "  0.28571428571428575,\n",
       "  0.15584415584415584,\n",
       "  0.08910891089108912,\n",
       "  0.062499999999999986,\n",
       "  0.1553398058252427,\n",
       "  0.0,\n",
       "  0.21818181818181817,\n",
       "  0.0,\n",
       "  0.21428571428571427,\n",
       "  0.21656050955414013,\n",
       "  0.05714285714285714,\n",
       "  0.2222222222222222,\n",
       "  0.028571428571428574,\n",
       "  0.2247191011235955,\n",
       "  0.07142857142857144,\n",
       "  0.0,\n",
       "  0.25742574257425743,\n",
       "  0.028985507246376812,\n",
       "  0.11538461538461539,\n",
       "  0.044444444444444446,\n",
       "  0.0,\n",
       "  0.43333333333333335,\n",
       "  0.15789473684210525,\n",
       "  0.16129032258064516,\n",
       "  0.5853658536585366,\n",
       "  0.20512820512820515,\n",
       "  0.1764705882352941,\n",
       "  0.11764705882352941,\n",
       "  0.0,\n",
       "  0.1702127659574468,\n",
       "  0.5050505050505051,\n",
       "  0.02564102564102564,\n",
       "  0.16666666666666666,\n",
       "  0.1694915254237288,\n",
       "  0.1935483870967742,\n",
       "  0.17721518987341772,\n",
       "  0.13636363636363635,\n",
       "  0.11570247933884298,\n",
       "  0.1797752808988764,\n",
       "  0.11023622047244094,\n",
       "  0.2736842105263158,\n",
       "  0.14754098360655737,\n",
       "  0.050632911392405056,\n",
       "  0.20000000000000004,\n",
       "  0.1818181818181818,\n",
       "  0.05454545454545455,\n",
       "  0.21782178217821782,\n",
       "  0.05555555555555555,\n",
       "  0.14285714285714285,\n",
       "  0.0,\n",
       "  0.02173913043478261,\n",
       "  0.19512195121951217,\n",
       "  0.04597701149425287,\n",
       "  0.027397260273972605,\n",
       "  0.3106796116504854,\n",
       "  0.1866666666666667,\n",
       "  0.186046511627907,\n",
       "  0.24528301886792453,\n",
       "  0.11023622047244094,\n",
       "  0.04761904761904762,\n",
       "  0.1348314606741573],\n",
       " 'rougeL': [0.18518518518518517,\n",
       "  0.3285714285714286,\n",
       "  0.27027027027027023,\n",
       "  0.1971830985915493,\n",
       "  0.2807017543859649,\n",
       "  0.13333333333333333,\n",
       "  0.35955056179775285,\n",
       "  0.19444444444444445,\n",
       "  0.3684210526315789,\n",
       "  0.16470588235294115,\n",
       "  0.27083333333333337,\n",
       "  0.4126984126984127,\n",
       "  0.17977528089887637,\n",
       "  0.1607142857142857,\n",
       "  0.4367816091954023,\n",
       "  0.2162162162162162,\n",
       "  0.15873015873015875,\n",
       "  0.2564102564102564,\n",
       "  0.6233766233766234,\n",
       "  0.2702702702702703,\n",
       "  0.2947368421052632,\n",
       "  0.6923076923076923,\n",
       "  0.2,\n",
       "  0.3116883116883117,\n",
       "  0.09615384615384616,\n",
       "  0.1818181818181818,\n",
       "  0.1437908496732026,\n",
       "  0.29565217391304344,\n",
       "  0.17777777777777776,\n",
       "  0.27848101265822783,\n",
       "  0.17142857142857143,\n",
       "  0.24,\n",
       "  0.2434782608695652,\n",
       "  0.13793103448275862,\n",
       "  0.37037037037037035,\n",
       "  0.21487603305785122,\n",
       "  0.2956521739130435,\n",
       "  0.32380952380952377,\n",
       "  0.19718309859154928,\n",
       "  0.29545454545454547,\n",
       "  0.28800000000000003,\n",
       "  0.5199999999999999,\n",
       "  0.3658536585365854,\n",
       "  0.15555555555555559,\n",
       "  0.14035087719298245,\n",
       "  0.24657534246575347,\n",
       "  0.22222222222222224,\n",
       "  0.20512820512820512,\n",
       "  0.547945205479452,\n",
       "  0.3,\n",
       "  0.31818181818181823,\n",
       "  0.1188118811881188,\n",
       "  1.0,\n",
       "  0.125,\n",
       "  0.2857142857142857,\n",
       "  0.16504854368932037,\n",
       "  0.0975609756097561,\n",
       "  0.23300970873786409,\n",
       "  0.1111111111111111,\n",
       "  0.19736842105263158,\n",
       "  0.1518987341772152,\n",
       "  0.14583333333333334,\n",
       "  0.1678321678321678,\n",
       "  0.2608695652173913,\n",
       "  0.20618556701030927,\n",
       "  0.39252336448598124,\n",
       "  0.22807017543859648,\n",
       "  0.6713286713286714,\n",
       "  0.0975609756097561,\n",
       "  0.2568807339449541,\n",
       "  0.3181818181818182,\n",
       "  0.06369426751592357,\n",
       "  0.14492753623188406,\n",
       "  0.3384615384615385,\n",
       "  0.3,\n",
       "  0.5263157894736842,\n",
       "  0.43373493975903615,\n",
       "  0.3235294117647059,\n",
       "  0.07894736842105263,\n",
       "  0.1836734693877551,\n",
       "  0.1935483870967742,\n",
       "  0.3269230769230769,\n",
       "  0.3550295857988166,\n",
       "  0.49315068493150693,\n",
       "  0.15384615384615385,\n",
       "  0.3220338983050847,\n",
       "  0.6666666666666666,\n",
       "  0.2750000000000001,\n",
       "  0.19753086419753085,\n",
       "  0.15730337078651688,\n",
       "  0.2588235294117647,\n",
       "  0.3826086956521739,\n",
       "  0.39473684210526316,\n",
       "  0.29473684210526313,\n",
       "  0.14,\n",
       "  0.1754385964912281,\n",
       "  0.304635761589404,\n",
       "  0.42666666666666664,\n",
       "  0.38,\n",
       "  0.15217391304347827,\n",
       "  0.27210884353741494,\n",
       "  0.12903225806451613,\n",
       "  0.21176470588235294,\n",
       "  0.37837837837837834,\n",
       "  0.21951219512195122,\n",
       "  0.3333333333333333,\n",
       "  0.07299270072992702,\n",
       "  0.3661971830985915,\n",
       "  0.23214285714285715,\n",
       "  0.2045454545454545,\n",
       "  0.14117647058823532,\n",
       "  0.41025641025641024,\n",
       "  0.18691588785046728,\n",
       "  0.20224719101123595,\n",
       "  0.3164556962025316,\n",
       "  0.31746031746031744,\n",
       "  0.35294117647058826,\n",
       "  0.2191780821917808,\n",
       "  0.18390804597701152,\n",
       "  0.32,\n",
       "  0.13333333333333333,\n",
       "  0.4,\n",
       "  0.27999999999999997,\n",
       "  0.2105263157894737,\n",
       "  0.32608695652173914,\n",
       "  0.21875000000000003,\n",
       "  0.17073170731707318,\n",
       "  0.3448275862068966,\n",
       "  0.25423728813559326,\n",
       "  0.1923076923076923,\n",
       "  0.08602150537634408,\n",
       "  0.14814814814814814,\n",
       "  0.1627906976744186,\n",
       "  0.25,\n",
       "  0.1935483870967742,\n",
       "  0.24324324324324326,\n",
       "  0.2361111111111111,\n",
       "  0.5,\n",
       "  0.2727272727272727,\n",
       "  0.37209302325581395,\n",
       "  0.2531645569620253,\n",
       "  0.18627450980392155,\n",
       "  0.16326530612244897,\n",
       "  0.2476190476190476,\n",
       "  0.1587301587301587,\n",
       "  0.3214285714285714,\n",
       "  0.08333333333333333,\n",
       "  0.3488372093023256,\n",
       "  0.37735849056603776,\n",
       "  0.19718309859154928,\n",
       "  0.35714285714285715,\n",
       "  0.11111111111111112,\n",
       "  0.2637362637362637,\n",
       "  0.2105263157894737,\n",
       "  0.06779661016949153,\n",
       "  0.2912621359223301,\n",
       "  0.11267605633802817,\n",
       "  0.22641509433962265,\n",
       "  0.1702127659574468,\n",
       "  0.05714285714285714,\n",
       "  0.4516129032258065,\n",
       "  0.2820512820512821,\n",
       "  0.28125000000000006,\n",
       "  0.6046511627906976,\n",
       "  0.2689075630252101,\n",
       "  0.31428571428571433,\n",
       "  0.19834710743801653,\n",
       "  0.044444444444444446,\n",
       "  0.2937062937062937,\n",
       "  0.6336633663366336,\n",
       "  0.09999999999999999,\n",
       "  0.34426229508196726,\n",
       "  0.2833333333333333,\n",
       "  0.3368421052631579,\n",
       "  0.3209876543209876,\n",
       "  0.24444444444444446,\n",
       "  0.17886178861788615,\n",
       "  0.32967032967032966,\n",
       "  0.23255813953488372,\n",
       "  0.3505154639175258,\n",
       "  0.2580645161290323,\n",
       "  0.22222222222222224,\n",
       "  0.36363636363636365,\n",
       "  0.22764227642276424,\n",
       "  0.14285714285714282,\n",
       "  0.1941747572815534,\n",
       "  0.20000000000000004,\n",
       "  0.23943661971830987,\n",
       "  0.08163265306122448,\n",
       "  0.1702127659574468,\n",
       "  0.21428571428571427,\n",
       "  0.1348314606741573,\n",
       "  0.10666666666666667,\n",
       "  0.4,\n",
       "  0.2894736842105263,\n",
       "  0.2900763358778626,\n",
       "  0.42592592592592593,\n",
       "  0.20155038759689925,\n",
       "  0.09302325581395349,\n",
       "  0.26373626373626374],\n",
       " 'rougeLsum': [0.18518518518518517,\n",
       "  0.3285714285714286,\n",
       "  0.27027027027027023,\n",
       "  0.1971830985915493,\n",
       "  0.2807017543859649,\n",
       "  0.13333333333333333,\n",
       "  0.35955056179775285,\n",
       "  0.19444444444444445,\n",
       "  0.3684210526315789,\n",
       "  0.16470588235294115,\n",
       "  0.27083333333333337,\n",
       "  0.4126984126984127,\n",
       "  0.17977528089887637,\n",
       "  0.1607142857142857,\n",
       "  0.4367816091954023,\n",
       "  0.2162162162162162,\n",
       "  0.15873015873015875,\n",
       "  0.2564102564102564,\n",
       "  0.6233766233766234,\n",
       "  0.2702702702702703,\n",
       "  0.2947368421052632,\n",
       "  0.6923076923076923,\n",
       "  0.2,\n",
       "  0.3116883116883117,\n",
       "  0.09615384615384616,\n",
       "  0.1818181818181818,\n",
       "  0.1437908496732026,\n",
       "  0.29565217391304344,\n",
       "  0.17777777777777776,\n",
       "  0.27848101265822783,\n",
       "  0.17142857142857143,\n",
       "  0.24,\n",
       "  0.2434782608695652,\n",
       "  0.13793103448275862,\n",
       "  0.37037037037037035,\n",
       "  0.21487603305785122,\n",
       "  0.2956521739130435,\n",
       "  0.32380952380952377,\n",
       "  0.19718309859154928,\n",
       "  0.29545454545454547,\n",
       "  0.28800000000000003,\n",
       "  0.5199999999999999,\n",
       "  0.3658536585365854,\n",
       "  0.15555555555555559,\n",
       "  0.14035087719298245,\n",
       "  0.24657534246575347,\n",
       "  0.22222222222222224,\n",
       "  0.20512820512820512,\n",
       "  0.547945205479452,\n",
       "  0.3,\n",
       "  0.31818181818181823,\n",
       "  0.1188118811881188,\n",
       "  1.0,\n",
       "  0.125,\n",
       "  0.2857142857142857,\n",
       "  0.16504854368932037,\n",
       "  0.0975609756097561,\n",
       "  0.23300970873786409,\n",
       "  0.1111111111111111,\n",
       "  0.19736842105263158,\n",
       "  0.1518987341772152,\n",
       "  0.14583333333333334,\n",
       "  0.1678321678321678,\n",
       "  0.2608695652173913,\n",
       "  0.20618556701030927,\n",
       "  0.39252336448598124,\n",
       "  0.22807017543859648,\n",
       "  0.6713286713286714,\n",
       "  0.0975609756097561,\n",
       "  0.2568807339449541,\n",
       "  0.3181818181818182,\n",
       "  0.06369426751592357,\n",
       "  0.14492753623188406,\n",
       "  0.3384615384615385,\n",
       "  0.3,\n",
       "  0.5263157894736842,\n",
       "  0.43373493975903615,\n",
       "  0.3235294117647059,\n",
       "  0.07894736842105263,\n",
       "  0.1836734693877551,\n",
       "  0.1935483870967742,\n",
       "  0.3269230769230769,\n",
       "  0.3550295857988166,\n",
       "  0.49315068493150693,\n",
       "  0.15384615384615385,\n",
       "  0.3220338983050847,\n",
       "  0.6666666666666666,\n",
       "  0.2750000000000001,\n",
       "  0.19753086419753085,\n",
       "  0.15730337078651688,\n",
       "  0.2588235294117647,\n",
       "  0.3826086956521739,\n",
       "  0.39473684210526316,\n",
       "  0.29473684210526313,\n",
       "  0.14,\n",
       "  0.1754385964912281,\n",
       "  0.304635761589404,\n",
       "  0.42666666666666664,\n",
       "  0.38,\n",
       "  0.15217391304347827,\n",
       "  0.27210884353741494,\n",
       "  0.12903225806451613,\n",
       "  0.21176470588235294,\n",
       "  0.37837837837837834,\n",
       "  0.21951219512195122,\n",
       "  0.3333333333333333,\n",
       "  0.07299270072992702,\n",
       "  0.3661971830985915,\n",
       "  0.23214285714285715,\n",
       "  0.2045454545454545,\n",
       "  0.14117647058823532,\n",
       "  0.41025641025641024,\n",
       "  0.18691588785046728,\n",
       "  0.20224719101123595,\n",
       "  0.3164556962025316,\n",
       "  0.31746031746031744,\n",
       "  0.35294117647058826,\n",
       "  0.2191780821917808,\n",
       "  0.18390804597701152,\n",
       "  0.32,\n",
       "  0.13333333333333333,\n",
       "  0.4,\n",
       "  0.27999999999999997,\n",
       "  0.2105263157894737,\n",
       "  0.32608695652173914,\n",
       "  0.21875000000000003,\n",
       "  0.17073170731707318,\n",
       "  0.3448275862068966,\n",
       "  0.25423728813559326,\n",
       "  0.1923076923076923,\n",
       "  0.08602150537634408,\n",
       "  0.14814814814814814,\n",
       "  0.1627906976744186,\n",
       "  0.25,\n",
       "  0.1935483870967742,\n",
       "  0.24324324324324326,\n",
       "  0.2361111111111111,\n",
       "  0.5,\n",
       "  0.2727272727272727,\n",
       "  0.37209302325581395,\n",
       "  0.2531645569620253,\n",
       "  0.18627450980392155,\n",
       "  0.16326530612244897,\n",
       "  0.2476190476190476,\n",
       "  0.1587301587301587,\n",
       "  0.3214285714285714,\n",
       "  0.08333333333333333,\n",
       "  0.3488372093023256,\n",
       "  0.37735849056603776,\n",
       "  0.19718309859154928,\n",
       "  0.35714285714285715,\n",
       "  0.11111111111111112,\n",
       "  0.2637362637362637,\n",
       "  0.2105263157894737,\n",
       "  0.06779661016949153,\n",
       "  0.2912621359223301,\n",
       "  0.11267605633802817,\n",
       "  0.22641509433962265,\n",
       "  0.1702127659574468,\n",
       "  0.05714285714285714,\n",
       "  0.4516129032258065,\n",
       "  0.2820512820512821,\n",
       "  0.28125000000000006,\n",
       "  0.6046511627906976,\n",
       "  0.2689075630252101,\n",
       "  0.31428571428571433,\n",
       "  0.19834710743801653,\n",
       "  0.044444444444444446,\n",
       "  0.2937062937062937,\n",
       "  0.6336633663366336,\n",
       "  0.09999999999999999,\n",
       "  0.34426229508196726,\n",
       "  0.2833333333333333,\n",
       "  0.3368421052631579,\n",
       "  0.3209876543209876,\n",
       "  0.24444444444444446,\n",
       "  0.17886178861788615,\n",
       "  0.32967032967032966,\n",
       "  0.23255813953488372,\n",
       "  0.3505154639175258,\n",
       "  0.2580645161290323,\n",
       "  0.22222222222222224,\n",
       "  0.36363636363636365,\n",
       "  0.22764227642276424,\n",
       "  0.14285714285714282,\n",
       "  0.1941747572815534,\n",
       "  0.20000000000000004,\n",
       "  0.23943661971830987,\n",
       "  0.08163265306122448,\n",
       "  0.1702127659574468,\n",
       "  0.21428571428571427,\n",
       "  0.1348314606741573,\n",
       "  0.10666666666666667,\n",
       "  0.4,\n",
       "  0.2894736842105263,\n",
       "  0.2900763358778626,\n",
       "  0.42592592592592593,\n",
       "  0.20155038759689925,\n",
       "  0.09302325581395349,\n",
       "  0.26373626373626374]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unaggregated results of inference on holdout set\n",
    "result_off_the_shelf_unagg = rouge.compute(predictions = holdout_off_the_shelf_summaries,\n",
    "                           references = holdout_article_summaries[:],\n",
    "                           use_stemmer=True,\n",
    "                             use_aggregator=False)\n",
    "\n",
    "result_off_the_shelf_unagg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce2ce4-f802-489b-b0f0-1ad21053825e",
   "metadata": {},
   "source": [
    "## Fine-tuning the t5-small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a2cea94-58f9-48d6-aba0-72981500bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a custom compute metrics function\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc1608f2-a700-46cc-9f1a-7c9760a66e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0393264293670654,\n",
       " 'eval_rouge1': 0.2204,\n",
       " 'eval_rouge2': 0.0916,\n",
       " 'eval_rougeL': 0.1819,\n",
       " 'eval_rougeLsum': 0.1816,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_runtime': 32.7618,\n",
       " 'eval_samples_per_second': 17.551,\n",
       " 'eval_steps_per_second': 0.275}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_small_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,#changed batch size from 16 to 32\n",
    "    per_device_eval_batch_size=32, #changed batch size from 16 to 32\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True, #even if we overtrain model by accident, we will still \n",
    "    #load the checkpoint that had lowest evaluation loss\n",
    "\n",
    "    #evaluation_strategy can be steps or epochs - correlates to how often we stop training and evaluate our model\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch' #save the model after every epoch\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_cnn_training_ds[\"train\"],\n",
    "    eval_dataset=tokenized_cnn_training_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "trainer.evaluate() #adding a max_new_tokens paramater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a121cc54-fee2-4b87-b73b-73f6c67e64ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 09:58, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.702605</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.674466</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.663713</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.660490</td>\n",
       "      <td>0.217600</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/gpu/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=144, training_loss=1.8958524068196614, metrics={'train_runtime': 602.271, 'train_samples_per_second': 15.249, 'train_steps_per_second': 0.239, 'total_flos': 2485958209437696.0, 'train_loss': 1.8958524068196614, 'epoch': 4.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_small_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, #changed batch size from 16 to 32\n",
    "    per_device_eval_batch_size=32,#changed batch size from 16 to 32\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True, #even if we overtrain model by accident, we will still \n",
    "    #load the checkpoint that had lowest evaluation loss\n",
    "\n",
    "    #evaluation_strategy can be steps or epochs - correlates to how often we stop training and evaluate our model\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch' #save the model after every epoch\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_cnn_training_ds[\"train\"],\n",
    "    eval_dataset=tokenized_cnn_training_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train() #adding a max_new_tokens paramater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d44c3b5c-3244-4600-bd73-1a934ee60f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d2c57c-2837-4e06-9349-c278555902fe",
   "metadata": {},
   "source": [
    "## Instantiating the fine_tuned model and running inference with that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16b57de0-c670-460a-9716-73343dad13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_checkpoint = \"t5_small_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56caed31-a345-4b65-83fe-ab02e480241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the fine-tuned model\n",
    "fine_tuned_model = T5ForConditionalGeneration.from_pretrained(fine_tuned_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0895a430-79fe-47dc-b20d-f894a470385e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46085037-19e1-4e1e-aac4-64424c3d6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the fine-tuned tokeinizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfc6a1-0da8-43b5-812c-40ad90bb3dfb",
   "metadata": {},
   "source": [
    "### Running inference on holdout set with \"fine-tuned\" t5-small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23b483df-7254-443e-a077-5060abcca7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a summarizer pipeline with the fine-tuned t5-small model\n",
    "fine_tuned_summarizer = pipeline ('summarization', model=fine_tuned_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9ce14-0a86-4d56-a47c-dd57b0e5cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 'holdout_article_texts' variable to hold articles from the test set\n",
    "holdout_article_texts = tokenized_cnn_holdout_ds[\"article\"]\n",
    "\n",
    "#creating 'holdout_article_summaries' variable to hold summaries from the test set\n",
    "holdout_article_summaries = tokenized_cnn_holdout_ds[\"highlights\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b652f63-0dc7-4bc0-8e3d-c2dba416dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2172 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████| 200/200 [04:50<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "#running inference on holdout set with fine-tuned\n",
    "from tqdm import tqdm\n",
    "\n",
    "#instantiating an empty list named 'holdout summaries'\n",
    "holdout_fine_tuned_summaries = []\n",
    "\n",
    "prefix ='summarize: '\n",
    "\n",
    "for i, text in enumerate(tqdm(holdout_article_texts[:500])):\n",
    "\n",
    "    candidate = fine_tuned_summarizer(prefix + text)\n",
    "\n",
    "    holdout_fine_tuned_summaries.append(candidate[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3b504a7-169f-461b-b5cd-dbfdbd0c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "      <th>t5_summaries</th>\n",
       "      <th>t5_fine_tuned_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manchester united have fallen off their perch....</td>\n",
       "      <td>manchester united were beaten 2-0 in their cha...</td>\n",
       "      <td>5b3a626078390cb0e05327b4019753fd11cb8cea</td>\n",
       "      <td>manchester united lost 1-0 to olympiacos in th...</td>\n",
       "      <td>manchester united lost 1-0 to olympiacos in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a mother whose russian husband snatched their ...</td>\n",
       "      <td>rachael neustadt's sons - daniel, eight and jo...</td>\n",
       "      <td>59d478d4a4299e2192997e56a9db9003fa2bac2d</td>\n",
       "      <td>rachael neustadt's sons daniel, eight, and jon...</td>\n",
       "      <td>rachael neustadt's sons daniel, eight, and jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim: supporters of mayor lutfur rahman alleg...</td>\n",
       "      <td>islamic voters allegedly told to be 'good musl...</td>\n",
       "      <td>ec961b7d0912e7753dffe4360b77481eba96f2e1</td>\n",
       "      <td>supporters of mayor lutfur rahman allegedly ha...</td>\n",
       "      <td>supporters of mayor lutfur rahman allegedly ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 15-year-old cousin of a palestinian boy wh...</td>\n",
       "      <td>mohammed abu khder, 16, abducted and burned to...</td>\n",
       "      <td>092d90d61eb105b3955820cc4894ac2c4995ad1b</td>\n",
       "      <td>mohammed abu khder, 16, was abducted from his ...</td>\n",
       "      <td>mohammed abu khder, 16, was burned to death in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it may have made its way up the pole to become...</td>\n",
       "      <td>spearmint rhino records £2.1m loss in 2011 .lo...</td>\n",
       "      <td>d0d59018cdf48aaeb6e1838c0323f8555e800765</td>\n",
       "      <td>spearmint rhino has recorded a loss of £2.1mil...</td>\n",
       "      <td>spearmint rhino has recorded a loss of £2.1m i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "      <td>'the block' caught out faking a visit from the...</td>\n",
       "      <td>985b1bf7fc710e4ffdd9dd02e72d889a7997e89d</td>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>the average cost of raising a child to seconda...</td>\n",
       "      <td>average cost of raising a child from birth up ...</td>\n",
       "      <td>e466296e19d7a14cf4916d70a2cbc296e4659c99</td>\n",
       "      <td>average cost of raising a child to secondary s...</td>\n",
       "      <td>average cost of raising a child to secondary s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>thai police investigating the murder of two br...</td>\n",
       "      <td>pornprasit sukdam claims he was offered £13,30...</td>\n",
       "      <td>a07624a84fe59a3321e83f153d6fd615207a8545</td>\n",
       "      <td>pornprasit sukdam claims he was offered 700,00...</td>\n",
       "      <td>pornprasit sukdam claims he was offered 700,00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>from clumpy flat shoes that seem to shorten a ...</td>\n",
       "      <td>clumpy flat shoes that seem to shorten a woman...</td>\n",
       "      <td>e16474b52bbf45f49434fc4a0b1d68e2d3fba3c3</td>\n",
       "      <td>kim carillo says she feels surprisingly sexy i...</td>\n",
       "      <td>kim carillo, who usually favours a more alluri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>for most people, a trip to the shops involves ...</td>\n",
       "      <td>moose spent seven hours dashing through the fi...</td>\n",
       "      <td>6f4bbcea20c6d6132b5d987912e06b5f6099a6ea</td>\n",
       "      <td>runaway elk caught dashing through streets of ...</td>\n",
       "      <td>runaway elk caught dashing through streets of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article  \\\n",
       "0    manchester united have fallen off their perch....   \n",
       "1    a mother whose russian husband snatched their ...   \n",
       "2    claim: supporters of mayor lutfur rahman alleg...   \n",
       "3    the 15-year-old cousin of a palestinian boy wh...   \n",
       "4    it may have made its way up the pole to become...   \n",
       "..                                                 ...   \n",
       "195  reality tv show the block has been accused of ...   \n",
       "196  the average cost of raising a child to seconda...   \n",
       "197  thai police investigating the murder of two br...   \n",
       "198  from clumpy flat shoes that seem to shorten a ...   \n",
       "199  for most people, a trip to the shops involves ...   \n",
       "\n",
       "                                            highlights  \\\n",
       "0    manchester united were beaten 2-0 in their cha...   \n",
       "1    rachael neustadt's sons - daniel, eight and jo...   \n",
       "2    islamic voters allegedly told to be 'good musl...   \n",
       "3    mohammed abu khder, 16, abducted and burned to...   \n",
       "4    spearmint rhino records £2.1m loss in 2011 .lo...   \n",
       "..                                                 ...   \n",
       "195  'the block' caught out faking a visit from the...   \n",
       "196  average cost of raising a child from birth up ...   \n",
       "197  pornprasit sukdam claims he was offered £13,30...   \n",
       "198  clumpy flat shoes that seem to shorten a woman...   \n",
       "199  moose spent seven hours dashing through the fi...   \n",
       "\n",
       "                                           id  \\\n",
       "0    5b3a626078390cb0e05327b4019753fd11cb8cea   \n",
       "1    59d478d4a4299e2192997e56a9db9003fa2bac2d   \n",
       "2    ec961b7d0912e7753dffe4360b77481eba96f2e1   \n",
       "3    092d90d61eb105b3955820cc4894ac2c4995ad1b   \n",
       "4    d0d59018cdf48aaeb6e1838c0323f8555e800765   \n",
       "..                                        ...   \n",
       "195  985b1bf7fc710e4ffdd9dd02e72d889a7997e89d   \n",
       "196  e466296e19d7a14cf4916d70a2cbc296e4659c99   \n",
       "197  a07624a84fe59a3321e83f153d6fd615207a8545   \n",
       "198  e16474b52bbf45f49434fc4a0b1d68e2d3fba3c3   \n",
       "199  6f4bbcea20c6d6132b5d987912e06b5f6099a6ea   \n",
       "\n",
       "                                          t5_summaries  \\\n",
       "0    manchester united lost 1-0 to olympiacos in th...   \n",
       "1    rachael neustadt's sons daniel, eight, and jon...   \n",
       "2    supporters of mayor lutfur rahman allegedly ha...   \n",
       "3    mohammed abu khder, 16, was abducted from his ...   \n",
       "4    spearmint rhino has recorded a loss of £2.1mil...   \n",
       "..                                                 ...   \n",
       "195  reality tv show the block has been accused of ...   \n",
       "196  average cost of raising a child to secondary s...   \n",
       "197  pornprasit sukdam claims he was offered 700,00...   \n",
       "198  kim carillo says she feels surprisingly sexy i...   \n",
       "199  runaway elk caught dashing through streets of ...   \n",
       "\n",
       "                               t5_fine_tuned_summaries  \n",
       "0    manchester united lost 1-0 to olympiacos in th...  \n",
       "1    rachael neustadt's sons daniel, eight, and jon...  \n",
       "2    supporters of mayor lutfur rahman allegedly ha...  \n",
       "3    mohammed abu khder, 16, was burned to death in...  \n",
       "4    spearmint rhino has recorded a loss of £2.1m i...  \n",
       "..                                                 ...  \n",
       "195  reality tv show the block has been accused of ...  \n",
       "196  average cost of raising a child to secondary s...  \n",
       "197  pornprasit sukdam claims he was offered 700,00...  \n",
       "198  kim carillo, who usually favours a more alluri...  \n",
       "199  runaway elk caught dashing through streets of ...  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_cnn_holdout_df['t5_fine_tuned_summaries'] = holdout_fine_tuned_summaries\n",
    "cleaned_cnn_holdout_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ecfdb-694d-475a-b291-53573928d4d6",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned t5 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63a48e5e-203d-4738-814f-bec45fff6c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.37165211524244746,\n",
       " 'rouge2': 0.1724968660090168,\n",
       " 'rougeL': 0.26523861858664466,\n",
       " 'rougeLsum': 0.2650452138248767}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregated results of inference on holdout set using fine-tune dmodel\n",
    "result_fine_tuned_agg = rouge.compute(predictions = holdout_fine_tuned_summaries,\n",
    "                           references = holdout_article_summaries[:],\n",
    "                           use_stemmer=True)\n",
    "\n",
    "result_fine_tuned_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11c5ccee-d1e5-4e41-b32d-58f9b1f3eb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': [0.28187919463087246,\n",
       "  0.36507936507936506,\n",
       "  0.4307692307692308,\n",
       "  0.4634146341463415,\n",
       "  0.5769230769230769,\n",
       "  0.23529411764705882,\n",
       "  0.4269662921348315,\n",
       "  0.3661971830985915,\n",
       "  0.2769230769230769,\n",
       "  0.3142857142857143,\n",
       "  0.41975308641975306,\n",
       "  0.46031746031746035,\n",
       "  0.35,\n",
       "  0.4948453608247423,\n",
       "  0.425531914893617,\n",
       "  0.2894736842105263,\n",
       "  0.3333333333333333,\n",
       "  0.4210526315789473,\n",
       "  0.6233766233766234,\n",
       "  0.3030303030303031,\n",
       "  0.4301075268817204,\n",
       "  0.7105263157894738,\n",
       "  0.28571428571428575,\n",
       "  0.4691358024691358,\n",
       "  0.2702702702702703,\n",
       "  0.2337662337662338,\n",
       "  0.45161290322580644,\n",
       "  0.4166666666666667,\n",
       "  0.29411764705882354,\n",
       "  0.6285714285714286,\n",
       "  0.30303030303030304,\n",
       "  0.3888888888888889,\n",
       "  0.3965517241379311,\n",
       "  0.21428571428571427,\n",
       "  0.25974025974025977,\n",
       "  0.2727272727272727,\n",
       "  0.45528455284552843,\n",
       "  0.4220183486238532,\n",
       "  0.4117647058823529,\n",
       "  0.4,\n",
       "  0.6614173228346456,\n",
       "  0.54,\n",
       "  0.46913580246913583,\n",
       "  0.3561643835616438,\n",
       "  0.41269841269841273,\n",
       "  0.3934426229508197,\n",
       "  0.38202247191011235,\n",
       "  0.31818181818181823,\n",
       "  0.7042253521126761,\n",
       "  0.4090909090909091,\n",
       "  0.45,\n",
       "  0.3076923076923077,\n",
       "  1.0,\n",
       "  0.2142857142857143,\n",
       "  0.12987012987012986,\n",
       "  0.23469387755102042,\n",
       "  0.16216216216216214,\n",
       "  0.3260869565217391,\n",
       "  0.20168067226890757,\n",
       "  0.23448275862068962,\n",
       "  0.3888888888888889,\n",
       "  0.35294117647058826,\n",
       "  0.38554216867469876,\n",
       "  0.3333333333333333,\n",
       "  0.34285714285714286,\n",
       "  0.4107142857142857,\n",
       "  0.3711340206185567,\n",
       "  0.6938775510204082,\n",
       "  0.22988505747126436,\n",
       "  0.43564356435643564,\n",
       "  0.3636363636363636,\n",
       "  0.11392405063291139,\n",
       "  0.411764705882353,\n",
       "  0.4153846153846154,\n",
       "  0.4571428571428571,\n",
       "  0.5894736842105263,\n",
       "  0.35185185185185186,\n",
       "  0.3055555555555555,\n",
       "  0.35955056179775274,\n",
       "  0.23853211009174313,\n",
       "  0.1818181818181818,\n",
       "  0.38596491228070173,\n",
       "  0.41420118343195267,\n",
       "  0.4444444444444444,\n",
       "  0.356687898089172,\n",
       "  0.40384615384615385,\n",
       "  1.0,\n",
       "  0.43478260869565216,\n",
       "  0.33766233766233766,\n",
       "  0.4210526315789474,\n",
       "  0.2857142857142857,\n",
       "  0.3846153846153846,\n",
       "  0.5789473684210527,\n",
       "  0.37209302325581395,\n",
       "  0.21978021978021978,\n",
       "  0.3529411764705882,\n",
       "  0.6187050359712231,\n",
       "  0.5952380952380953,\n",
       "  0.46464646464646464,\n",
       "  0.17582417582417584,\n",
       "  0.2074074074074074,\n",
       "  0.2909090909090909,\n",
       "  0.30555555555555564,\n",
       "  0.5121951219512195,\n",
       "  0.2564102564102564,\n",
       "  0.48214285714285715,\n",
       "  0.17600000000000002,\n",
       "  0.5542168674698795,\n",
       "  0.396039603960396,\n",
       "  0.2828282828282828,\n",
       "  0.1875,\n",
       "  0.4032258064516129,\n",
       "  0.42105263157894735,\n",
       "  0.2105263157894737,\n",
       "  0.3673469387755102,\n",
       "  0.26229508196721313,\n",
       "  0.29411764705882354,\n",
       "  0.24691358024691357,\n",
       "  0.25,\n",
       "  0.47154471544715454,\n",
       "  0.17391304347826086,\n",
       "  0.3950617283950617,\n",
       "  0.49438202247191004,\n",
       "  0.4406779661016949,\n",
       "  0.38202247191011235,\n",
       "  0.31428571428571433,\n",
       "  0.3466666666666667,\n",
       "  0.37681159420289856,\n",
       "  0.23913043478260868,\n",
       "  0.2857142857142857,\n",
       "  0.13953488372093023,\n",
       "  0.21428571428571425,\n",
       "  0.27450980392156865,\n",
       "  0.36363636363636365,\n",
       "  0.13793103448275865,\n",
       "  0.3888888888888889,\n",
       "  0.3666666666666667,\n",
       "  0.5970149253731343,\n",
       "  0.44642857142857145,\n",
       "  0.42553191489361697,\n",
       "  0.19718309859154928,\n",
       "  0.16842105263157894,\n",
       "  0.23809523809523808,\n",
       "  0.3614457831325302,\n",
       "  0.1132075471698113,\n",
       "  0.5054945054945055,\n",
       "  0.07999999999999999,\n",
       "  0.26506024096385544,\n",
       "  0.5316455696202532,\n",
       "  0.25862068965517243,\n",
       "  0.30985915492957744,\n",
       "  0.21875,\n",
       "  0.40404040404040403,\n",
       "  0.37499999999999994,\n",
       "  0.18181818181818182,\n",
       "  0.45569620253164556,\n",
       "  0.1791044776119403,\n",
       "  0.4523809523809524,\n",
       "  0.4307692307692308,\n",
       "  0.37681159420289856,\n",
       "  0.4516129032258065,\n",
       "  0.2727272727272727,\n",
       "  0.38202247191011235,\n",
       "  0.6046511627906976,\n",
       "  0.45801526717557256,\n",
       "  0.36065573770491804,\n",
       "  0.41666666666666663,\n",
       "  0.04545454545454545,\n",
       "  0.36619718309859156,\n",
       "  0.6019417475728156,\n",
       "  0.22784810126582278,\n",
       "  0.5079365079365079,\n",
       "  0.3619047619047619,\n",
       "  0.4356435643564357,\n",
       "  0.409090909090909,\n",
       "  0.4424778761061947,\n",
       "  0.32592592592592595,\n",
       "  0.36363636363636365,\n",
       "  0.4305555555555556,\n",
       "  0.3225806451612903,\n",
       "  0.42857142857142855,\n",
       "  0.3103448275862069,\n",
       "  0.41379310344827586,\n",
       "  0.5210084033613446,\n",
       "  0.34210526315789475,\n",
       "  0.2528735632183908,\n",
       "  0.2857142857142857,\n",
       "  0.3801652892561983,\n",
       "  0.30434782608695654,\n",
       "  0.48421052631578954,\n",
       "  0.49090909090909085,\n",
       "  0.33333333333333337,\n",
       "  0.19444444444444445,\n",
       "  0.4044943820224719,\n",
       "  0.31404958677685946,\n",
       "  0.4444444444444445,\n",
       "  0.6407766990291263,\n",
       "  0.3888888888888889,\n",
       "  0.4,\n",
       "  0.375],\n",
       " 'rouge2': [0.08163265306122448,\n",
       "  0.2419354838709677,\n",
       "  0.12698412698412698,\n",
       "  0.13580246913580246,\n",
       "  0.32,\n",
       "  0.08163265306122448,\n",
       "  0.2758620689655172,\n",
       "  0.2608695652173913,\n",
       "  0.19047619047619047,\n",
       "  0.058823529411764705,\n",
       "  0.20253164556962022,\n",
       "  0.3709677419354839,\n",
       "  0.1282051282051282,\n",
       "  0.3157894736842105,\n",
       "  0.2391304347826087,\n",
       "  0.08108108108108107,\n",
       "  0.19230769230769232,\n",
       "  0.17204301075268816,\n",
       "  0.5066666666666667,\n",
       "  0.020618556701030927,\n",
       "  0.2637362637362637,\n",
       "  0.6216216216216217,\n",
       "  0.08823529411764706,\n",
       "  0.3037974683544304,\n",
       "  0.12844036697247707,\n",
       "  0.10666666666666667,\n",
       "  0.18300653594771243,\n",
       "  0.288135593220339,\n",
       "  0.07999999999999999,\n",
       "  0.38235294117647056,\n",
       "  0.08247422680412371,\n",
       "  0.2,\n",
       "  0.07017543859649122,\n",
       "  0.07407407407407407,\n",
       "  0.0,\n",
       "  0.030769230769230767,\n",
       "  0.1818181818181818,\n",
       "  0.1495327102803738,\n",
       "  0.21212121212121215,\n",
       "  0.22727272727272727,\n",
       "  0.576,\n",
       "  0.5102040816326531,\n",
       "  0.20253164556962028,\n",
       "  0.11267605633802817,\n",
       "  0.14516129032258066,\n",
       "  0.1694915254237288,\n",
       "  0.18390804597701146,\n",
       "  0.0,\n",
       "  0.49275362318840576,\n",
       "  0.18604651162790697,\n",
       "  0.15384615384615385,\n",
       "  0.06741573033707865,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09278350515463918,\n",
       "  0.0,\n",
       "  0.08888888888888888,\n",
       "  0.03418803418803419,\n",
       "  0.13986013986013987,\n",
       "  0.14285714285714288,\n",
       "  0.039999999999999994,\n",
       "  0.06097560975609756,\n",
       "  0.2413793103448276,\n",
       "  0.13592233009708737,\n",
       "  0.14545454545454545,\n",
       "  0.18947368421052632,\n",
       "  0.6620689655172413,\n",
       "  0.023529411764705882,\n",
       "  0.20202020202020202,\n",
       "  0.11627906976744186,\n",
       "  0.0,\n",
       "  0.0909090909090909,\n",
       "  0.125,\n",
       "  0.29411764705882354,\n",
       "  0.3870967741935484,\n",
       "  0.1509433962264151,\n",
       "  0.0857142857142857,\n",
       "  0.18390804597701146,\n",
       "  0.07476635514018691,\n",
       "  0.10666666666666667,\n",
       "  0.19642857142857145,\n",
       "  0.26347305389221554,\n",
       "  0.22857142857142856,\n",
       "  0.05161290322580645,\n",
       "  0.1568627450980392,\n",
       "  1.0,\n",
       "  0.208955223880597,\n",
       "  0.13333333333333333,\n",
       "  0.15053763440860218,\n",
       "  0.125,\n",
       "  0.2745098039215686,\n",
       "  0.29729729729729726,\n",
       "  0.2142857142857143,\n",
       "  0.11235955056179774,\n",
       "  0.0816326530612245,\n",
       "  0.30656934306569344,\n",
       "  0.46341463414634143,\n",
       "  0.16494845360824742,\n",
       "  0.0,\n",
       "  0.04511278195488722,\n",
       "  0.037037037037037035,\n",
       "  0.14285714285714285,\n",
       "  0.32098765432098764,\n",
       "  0.0,\n",
       "  0.2727272727272727,\n",
       "  0.0,\n",
       "  0.3209876543209876,\n",
       "  0.24242424242424243,\n",
       "  0.06185567010309278,\n",
       "  0.0,\n",
       "  0.2459016393442623,\n",
       "  0.27956989247311825,\n",
       "  0.021505376344086023,\n",
       "  0.20689655172413796,\n",
       "  0.06779661016949153,\n",
       "  0.13999999999999999,\n",
       "  0.025316455696202528,\n",
       "  0.06976744186046512,\n",
       "  0.23140495867768596,\n",
       "  0.05970149253731343,\n",
       "  0.12658227848101267,\n",
       "  0.3908045977011494,\n",
       "  0.13793103448275862,\n",
       "  0.20689655172413793,\n",
       "  0.029411764705882356,\n",
       "  0.0821917808219178,\n",
       "  0.20895522388059704,\n",
       "  0.08888888888888889,\n",
       "  0.0975609756097561,\n",
       "  0.0,\n",
       "  0.03703703703703704,\n",
       "  0.06,\n",
       "  0.08247422680412371,\n",
       "  0.0,\n",
       "  0.17142857142857143,\n",
       "  0.13559322033898308,\n",
       "  0.3692307692307692,\n",
       "  0.21818181818181817,\n",
       "  0.1956521739130435,\n",
       "  0.028985507246376812,\n",
       "  0.05319148936170213,\n",
       "  0.04878048780487805,\n",
       "  0.17283950617283952,\n",
       "  0.0,\n",
       "  0.24719101123595502,\n",
       "  0.04166666666666667,\n",
       "  0.07407407407407407,\n",
       "  0.17948717948717952,\n",
       "  0.017543859649122803,\n",
       "  0.17391304347826086,\n",
       "  0.032258064516129024,\n",
       "  0.1443298969072165,\n",
       "  0.10638297872340426,\n",
       "  0.0,\n",
       "  0.31168831168831174,\n",
       "  0.030769230769230767,\n",
       "  0.04878048780487805,\n",
       "  0.19047619047619047,\n",
       "  0.26865671641791045,\n",
       "  0.43333333333333335,\n",
       "  0.06976744186046512,\n",
       "  0.11494252873563218,\n",
       "  0.5853658536585366,\n",
       "  0.248062015503876,\n",
       "  0.2033898305084746,\n",
       "  0.11864406779661019,\n",
       "  0.0,\n",
       "  0.08571428571428572,\n",
       "  0.41584158415841577,\n",
       "  0.05194805194805195,\n",
       "  0.3064516129032258,\n",
       "  0.1359223300970874,\n",
       "  0.2626262626262626,\n",
       "  0.20930232558139536,\n",
       "  0.1081081081081081,\n",
       "  0.12030075187969924,\n",
       "  0.2558139534883721,\n",
       "  0.22535211267605634,\n",
       "  0.06593406593406594,\n",
       "  0.19999999999999998,\n",
       "  0.07142857142857144,\n",
       "  0.1929824561403509,\n",
       "  0.3076923076923077,\n",
       "  0.09333333333333334,\n",
       "  0.047058823529411764,\n",
       "  0.06741573033707865,\n",
       "  0.10084033613445378,\n",
       "  0.08888888888888889,\n",
       "  0.30107526881720426,\n",
       "  0.2592592592592593,\n",
       "  0.05660377358490566,\n",
       "  0.02857142857142857,\n",
       "  0.1379310344827586,\n",
       "  0.15126050420168066,\n",
       "  0.18867924528301885,\n",
       "  0.3762376237623763,\n",
       "  0.16901408450704225,\n",
       "  0.2795698924731183,\n",
       "  0.12765957446808512],\n",
       " 'rougeL': [0.1879194630872483,\n",
       "  0.3333333333333333,\n",
       "  0.276923076923077,\n",
       "  0.21951219512195122,\n",
       "  0.5384615384615384,\n",
       "  0.1568627450980392,\n",
       "  0.38202247191011235,\n",
       "  0.3380281690140845,\n",
       "  0.2769230769230769,\n",
       "  0.2,\n",
       "  0.2716049382716049,\n",
       "  0.4126984126984127,\n",
       "  0.25,\n",
       "  0.3711340206185567,\n",
       "  0.3617021276595745,\n",
       "  0.15789473684210528,\n",
       "  0.2962962962962963,\n",
       "  0.2736842105263158,\n",
       "  0.6233766233766234,\n",
       "  0.1818181818181818,\n",
       "  0.27956989247311825,\n",
       "  0.7105263157894738,\n",
       "  0.2,\n",
       "  0.4197530864197531,\n",
       "  0.1981981981981982,\n",
       "  0.15584415584415584,\n",
       "  0.3096774193548387,\n",
       "  0.35000000000000003,\n",
       "  0.1568627450980392,\n",
       "  0.2857142857142857,\n",
       "  0.18181818181818182,\n",
       "  0.3333333333333333,\n",
       "  0.2068965517241379,\n",
       "  0.10714285714285714,\n",
       "  0.12987012987012989,\n",
       "  0.16666666666666669,\n",
       "  0.26016260162601623,\n",
       "  0.3119266055045872,\n",
       "  0.20588235294117646,\n",
       "  0.31111111111111117,\n",
       "  0.6456692913385826,\n",
       "  0.5199999999999999,\n",
       "  0.34567901234567905,\n",
       "  0.19178082191780824,\n",
       "  0.25396825396825395,\n",
       "  0.29508196721311475,\n",
       "  0.20224719101123595,\n",
       "  0.22727272727272727,\n",
       "  0.5633802816901409,\n",
       "  0.2954545454545454,\n",
       "  0.3,\n",
       "  0.19780219780219782,\n",
       "  1.0,\n",
       "  0.14285714285714285,\n",
       "  0.0779220779220779,\n",
       "  0.173469387755102,\n",
       "  0.10810810810810811,\n",
       "  0.17391304347826086,\n",
       "  0.11764705882352941,\n",
       "  0.16551724137931034,\n",
       "  0.16666666666666666,\n",
       "  0.17647058823529413,\n",
       "  0.18072289156626503,\n",
       "  0.3,\n",
       "  0.28571428571428575,\n",
       "  0.26785714285714285,\n",
       "  0.24742268041237114,\n",
       "  0.6938775510204082,\n",
       "  0.11494252873563218,\n",
       "  0.27722772277227725,\n",
       "  0.22727272727272727,\n",
       "  0.05063291139240506,\n",
       "  0.23529411764705885,\n",
       "  0.24615384615384617,\n",
       "  0.2857142857142857,\n",
       "  0.46315789473684216,\n",
       "  0.2962962962962962,\n",
       "  0.25,\n",
       "  0.29213483146067415,\n",
       "  0.16513761467889912,\n",
       "  0.1818181818181818,\n",
       "  0.29824561403508776,\n",
       "  0.3550295857988166,\n",
       "  0.2777777777777778,\n",
       "  0.14012738853503184,\n",
       "  0.25000000000000006,\n",
       "  1.0,\n",
       "  0.40579710144927533,\n",
       "  0.18181818181818182,\n",
       "  0.2105263157894737,\n",
       "  0.24489795918367346,\n",
       "  0.34615384615384615,\n",
       "  0.39473684210526316,\n",
       "  0.34883720930232553,\n",
       "  0.15384615384615383,\n",
       "  0.2745098039215686,\n",
       "  0.3884892086330935,\n",
       "  0.3333333333333333,\n",
       "  0.3434343434343434,\n",
       "  0.08791208791208792,\n",
       "  0.1037037037037037,\n",
       "  0.16363636363636364,\n",
       "  0.19444444444444445,\n",
       "  0.43902439024390244,\n",
       "  0.17948717948717946,\n",
       "  0.4285714285714285,\n",
       "  0.07999999999999999,\n",
       "  0.2891566265060241,\n",
       "  0.23762376237623764,\n",
       "  0.2222222222222222,\n",
       "  0.125,\n",
       "  0.22580645161290325,\n",
       "  0.18947368421052632,\n",
       "  0.12631578947368421,\n",
       "  0.27210884353741494,\n",
       "  0.19672131147540986,\n",
       "  0.2156862745098039,\n",
       "  0.1728395061728395,\n",
       "  0.15909090909090912,\n",
       "  0.35772357723577236,\n",
       "  0.17391304347826086,\n",
       "  0.19753086419753085,\n",
       "  0.29213483146067415,\n",
       "  0.288135593220339,\n",
       "  0.2696629213483146,\n",
       "  0.17142857142857143,\n",
       "  0.24000000000000005,\n",
       "  0.2898550724637681,\n",
       "  0.21739130434782605,\n",
       "  0.1904761904761905,\n",
       "  0.06976744186046512,\n",
       "  0.14285714285714285,\n",
       "  0.2156862745098039,\n",
       "  0.2828282828282829,\n",
       "  0.03448275862068966,\n",
       "  0.25,\n",
       "  0.23333333333333334,\n",
       "  0.5074626865671642,\n",
       "  0.26785714285714285,\n",
       "  0.3191489361702128,\n",
       "  0.1408450704225352,\n",
       "  0.11578947368421053,\n",
       "  0.11904761904761904,\n",
       "  0.21686746987951808,\n",
       "  0.03773584905660378,\n",
       "  0.21978021978021978,\n",
       "  0.07999999999999999,\n",
       "  0.19277108433734938,\n",
       "  0.3164556962025316,\n",
       "  0.12068965517241378,\n",
       "  0.2816901408450704,\n",
       "  0.15625,\n",
       "  0.26262626262626265,\n",
       "  0.20833333333333334,\n",
       "  0.09090909090909091,\n",
       "  0.37974683544303794,\n",
       "  0.14925373134328357,\n",
       "  0.16666666666666666,\n",
       "  0.3384615384615385,\n",
       "  0.3478260869565218,\n",
       "  0.4516129032258065,\n",
       "  0.20454545454545456,\n",
       "  0.2696629213483146,\n",
       "  0.6046511627906976,\n",
       "  0.32061068702290074,\n",
       "  0.29508196721311475,\n",
       "  0.21666666666666665,\n",
       "  0.04545454545454545,\n",
       "  0.2535211267605634,\n",
       "  0.5631067961165048,\n",
       "  0.10126582278481011,\n",
       "  0.365079365079365,\n",
       "  0.32380952380952377,\n",
       "  0.396039603960396,\n",
       "  0.3181818181818182,\n",
       "  0.21238938053097342,\n",
       "  0.17777777777777778,\n",
       "  0.2727272727272727,\n",
       "  0.3194444444444444,\n",
       "  0.17204301075268816,\n",
       "  0.3214285714285714,\n",
       "  0.20689655172413793,\n",
       "  0.3103448275862069,\n",
       "  0.3865546218487395,\n",
       "  0.1842105263157895,\n",
       "  0.16091954022988506,\n",
       "  0.2417582417582418,\n",
       "  0.19834710743801653,\n",
       "  0.1956521739130435,\n",
       "  0.4,\n",
       "  0.2363636363636364,\n",
       "  0.14814814814814814,\n",
       "  0.1111111111111111,\n",
       "  0.29213483146067415,\n",
       "  0.19834710743801653,\n",
       "  0.3148148148148148,\n",
       "  0.5825242718446602,\n",
       "  0.23611111111111113,\n",
       "  0.35789473684210527,\n",
       "  0.22916666666666669],\n",
       " 'rougeLsum': [0.1879194630872483,\n",
       "  0.3333333333333333,\n",
       "  0.276923076923077,\n",
       "  0.21951219512195122,\n",
       "  0.5384615384615384,\n",
       "  0.1568627450980392,\n",
       "  0.38202247191011235,\n",
       "  0.3380281690140845,\n",
       "  0.2769230769230769,\n",
       "  0.2,\n",
       "  0.2716049382716049,\n",
       "  0.4126984126984127,\n",
       "  0.25,\n",
       "  0.3711340206185567,\n",
       "  0.3617021276595745,\n",
       "  0.15789473684210528,\n",
       "  0.2962962962962963,\n",
       "  0.2736842105263158,\n",
       "  0.6233766233766234,\n",
       "  0.1818181818181818,\n",
       "  0.27956989247311825,\n",
       "  0.7105263157894738,\n",
       "  0.2,\n",
       "  0.4197530864197531,\n",
       "  0.1981981981981982,\n",
       "  0.15584415584415584,\n",
       "  0.3096774193548387,\n",
       "  0.35000000000000003,\n",
       "  0.1568627450980392,\n",
       "  0.2857142857142857,\n",
       "  0.18181818181818182,\n",
       "  0.3333333333333333,\n",
       "  0.2068965517241379,\n",
       "  0.10714285714285714,\n",
       "  0.12987012987012989,\n",
       "  0.16666666666666669,\n",
       "  0.26016260162601623,\n",
       "  0.3119266055045872,\n",
       "  0.20588235294117646,\n",
       "  0.31111111111111117,\n",
       "  0.6456692913385826,\n",
       "  0.5199999999999999,\n",
       "  0.34567901234567905,\n",
       "  0.19178082191780824,\n",
       "  0.25396825396825395,\n",
       "  0.29508196721311475,\n",
       "  0.20224719101123595,\n",
       "  0.22727272727272727,\n",
       "  0.5633802816901409,\n",
       "  0.2954545454545454,\n",
       "  0.3,\n",
       "  0.19780219780219782,\n",
       "  1.0,\n",
       "  0.14285714285714285,\n",
       "  0.0779220779220779,\n",
       "  0.173469387755102,\n",
       "  0.10810810810810811,\n",
       "  0.17391304347826086,\n",
       "  0.11764705882352941,\n",
       "  0.16551724137931034,\n",
       "  0.16666666666666666,\n",
       "  0.17647058823529413,\n",
       "  0.18072289156626503,\n",
       "  0.3,\n",
       "  0.28571428571428575,\n",
       "  0.26785714285714285,\n",
       "  0.24742268041237114,\n",
       "  0.6938775510204082,\n",
       "  0.11494252873563218,\n",
       "  0.27722772277227725,\n",
       "  0.22727272727272727,\n",
       "  0.05063291139240506,\n",
       "  0.23529411764705885,\n",
       "  0.24615384615384617,\n",
       "  0.2857142857142857,\n",
       "  0.46315789473684216,\n",
       "  0.2962962962962962,\n",
       "  0.25,\n",
       "  0.29213483146067415,\n",
       "  0.16513761467889912,\n",
       "  0.1818181818181818,\n",
       "  0.29824561403508776,\n",
       "  0.3550295857988166,\n",
       "  0.2777777777777778,\n",
       "  0.14012738853503184,\n",
       "  0.25000000000000006,\n",
       "  1.0,\n",
       "  0.40579710144927533,\n",
       "  0.18181818181818182,\n",
       "  0.2105263157894737,\n",
       "  0.24489795918367346,\n",
       "  0.34615384615384615,\n",
       "  0.39473684210526316,\n",
       "  0.34883720930232553,\n",
       "  0.15384615384615383,\n",
       "  0.2745098039215686,\n",
       "  0.3884892086330935,\n",
       "  0.3333333333333333,\n",
       "  0.3434343434343434,\n",
       "  0.08791208791208792,\n",
       "  0.1037037037037037,\n",
       "  0.16363636363636364,\n",
       "  0.19444444444444445,\n",
       "  0.43902439024390244,\n",
       "  0.17948717948717946,\n",
       "  0.4285714285714285,\n",
       "  0.07999999999999999,\n",
       "  0.2891566265060241,\n",
       "  0.23762376237623764,\n",
       "  0.2222222222222222,\n",
       "  0.125,\n",
       "  0.22580645161290325,\n",
       "  0.18947368421052632,\n",
       "  0.12631578947368421,\n",
       "  0.27210884353741494,\n",
       "  0.19672131147540986,\n",
       "  0.2156862745098039,\n",
       "  0.1728395061728395,\n",
       "  0.15909090909090912,\n",
       "  0.35772357723577236,\n",
       "  0.17391304347826086,\n",
       "  0.19753086419753085,\n",
       "  0.29213483146067415,\n",
       "  0.288135593220339,\n",
       "  0.2696629213483146,\n",
       "  0.17142857142857143,\n",
       "  0.24000000000000005,\n",
       "  0.2898550724637681,\n",
       "  0.21739130434782605,\n",
       "  0.1904761904761905,\n",
       "  0.06976744186046512,\n",
       "  0.14285714285714285,\n",
       "  0.2156862745098039,\n",
       "  0.2828282828282829,\n",
       "  0.03448275862068966,\n",
       "  0.25,\n",
       "  0.23333333333333334,\n",
       "  0.5074626865671642,\n",
       "  0.26785714285714285,\n",
       "  0.3191489361702128,\n",
       "  0.1408450704225352,\n",
       "  0.11578947368421053,\n",
       "  0.11904761904761904,\n",
       "  0.21686746987951808,\n",
       "  0.03773584905660378,\n",
       "  0.21978021978021978,\n",
       "  0.07999999999999999,\n",
       "  0.19277108433734938,\n",
       "  0.3164556962025316,\n",
       "  0.12068965517241378,\n",
       "  0.2816901408450704,\n",
       "  0.15625,\n",
       "  0.26262626262626265,\n",
       "  0.20833333333333334,\n",
       "  0.09090909090909091,\n",
       "  0.37974683544303794,\n",
       "  0.14925373134328357,\n",
       "  0.16666666666666666,\n",
       "  0.3384615384615385,\n",
       "  0.3478260869565218,\n",
       "  0.4516129032258065,\n",
       "  0.20454545454545456,\n",
       "  0.2696629213483146,\n",
       "  0.6046511627906976,\n",
       "  0.32061068702290074,\n",
       "  0.29508196721311475,\n",
       "  0.21666666666666665,\n",
       "  0.04545454545454545,\n",
       "  0.2535211267605634,\n",
       "  0.5631067961165048,\n",
       "  0.10126582278481011,\n",
       "  0.365079365079365,\n",
       "  0.32380952380952377,\n",
       "  0.396039603960396,\n",
       "  0.3181818181818182,\n",
       "  0.21238938053097342,\n",
       "  0.17777777777777778,\n",
       "  0.2727272727272727,\n",
       "  0.3194444444444444,\n",
       "  0.17204301075268816,\n",
       "  0.3214285714285714,\n",
       "  0.20689655172413793,\n",
       "  0.3103448275862069,\n",
       "  0.3865546218487395,\n",
       "  0.1842105263157895,\n",
       "  0.16091954022988506,\n",
       "  0.2417582417582418,\n",
       "  0.19834710743801653,\n",
       "  0.1956521739130435,\n",
       "  0.4,\n",
       "  0.2363636363636364,\n",
       "  0.14814814814814814,\n",
       "  0.1111111111111111,\n",
       "  0.29213483146067415,\n",
       "  0.19834710743801653,\n",
       "  0.3148148148148148,\n",
       "  0.5825242718446602,\n",
       "  0.23611111111111113,\n",
       "  0.35789473684210527,\n",
       "  0.22916666666666669]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unaggregated results of inference on holdout set using fine-tune dmodel\n",
    "result_fine_tuned_unagg = rouge.compute(predictions = holdout_fine_tuned_summaries,\n",
    "                           references = holdout_article_summaries[:],\n",
    "                           use_stemmer=True,\n",
    "                            use_aggregator=False)\n",
    "                                      \n",
    "\n",
    "result_fine_tuned_unagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0432edd-dba5-4a0c-aa30-0b4f311a2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_cnn_holdout_df.to_csv('data/data_frame_after_t5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5586faf2-62ca-47e5-819c-e131f7fb52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "      <th>t5_summaries</th>\n",
       "      <th>t5_fine_tuned_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>manchester united have fallen off their perch....</td>\n",
       "      <td>manchester united were beaten 2-0 in their cha...</td>\n",
       "      <td>5b3a626078390cb0e05327b4019753fd11cb8cea</td>\n",
       "      <td>manchester united lost 1-0 to olympiacos in th...</td>\n",
       "      <td>manchester united lost 1-0 to olympiacos in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a mother whose russian husband snatched their ...</td>\n",
       "      <td>rachael neustadt's sons - daniel, eight and jo...</td>\n",
       "      <td>59d478d4a4299e2192997e56a9db9003fa2bac2d</td>\n",
       "      <td>rachael neustadt's sons daniel, eight, and jon...</td>\n",
       "      <td>rachael neustadt's sons daniel, eight, and jon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>claim: supporters of mayor lutfur rahman alleg...</td>\n",
       "      <td>islamic voters allegedly told to be 'good musl...</td>\n",
       "      <td>ec961b7d0912e7753dffe4360b77481eba96f2e1</td>\n",
       "      <td>supporters of mayor lutfur rahman allegedly ha...</td>\n",
       "      <td>supporters of mayor lutfur rahman allegedly ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the 15-year-old cousin of a palestinian boy wh...</td>\n",
       "      <td>mohammed abu khder, 16, abducted and burned to...</td>\n",
       "      <td>092d90d61eb105b3955820cc4894ac2c4995ad1b</td>\n",
       "      <td>mohammed abu khder, 16, was abducted from his ...</td>\n",
       "      <td>mohammed abu khder, 16, was burned to death in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>it may have made its way up the pole to become...</td>\n",
       "      <td>spearmint rhino records £2.1m loss in 2011 .lo...</td>\n",
       "      <td>d0d59018cdf48aaeb6e1838c0323f8555e800765</td>\n",
       "      <td>spearmint rhino has recorded a loss of £2.1mil...</td>\n",
       "      <td>spearmint rhino has recorded a loss of £2.1m i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "      <td>'the block' caught out faking a visit from the...</td>\n",
       "      <td>985b1bf7fc710e4ffdd9dd02e72d889a7997e89d</td>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "      <td>reality tv show the block has been accused of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>the average cost of raising a child to seconda...</td>\n",
       "      <td>average cost of raising a child from birth up ...</td>\n",
       "      <td>e466296e19d7a14cf4916d70a2cbc296e4659c99</td>\n",
       "      <td>average cost of raising a child to secondary s...</td>\n",
       "      <td>average cost of raising a child to secondary s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>thai police investigating the murder of two br...</td>\n",
       "      <td>pornprasit sukdam claims he was offered £13,30...</td>\n",
       "      <td>a07624a84fe59a3321e83f153d6fd615207a8545</td>\n",
       "      <td>pornprasit sukdam claims he was offered 700,00...</td>\n",
       "      <td>pornprasit sukdam claims he was offered 700,00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>from clumpy flat shoes that seem to shorten a ...</td>\n",
       "      <td>clumpy flat shoes that seem to shorten a woman...</td>\n",
       "      <td>e16474b52bbf45f49434fc4a0b1d68e2d3fba3c3</td>\n",
       "      <td>kim carillo says she feels surprisingly sexy i...</td>\n",
       "      <td>kim carillo, who usually favours a more alluri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>for most people, a trip to the shops involves ...</td>\n",
       "      <td>moose spent seven hours dashing through the fi...</td>\n",
       "      <td>6f4bbcea20c6d6132b5d987912e06b5f6099a6ea</td>\n",
       "      <td>runaway elk caught dashing through streets of ...</td>\n",
       "      <td>runaway elk caught dashing through streets of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            article  \\\n",
       "0             0  manchester united have fallen off their perch....   \n",
       "1             1  a mother whose russian husband snatched their ...   \n",
       "2             2  claim: supporters of mayor lutfur rahman alleg...   \n",
       "3             3  the 15-year-old cousin of a palestinian boy wh...   \n",
       "4             4  it may have made its way up the pole to become...   \n",
       "..          ...                                                ...   \n",
       "195         195  reality tv show the block has been accused of ...   \n",
       "196         196  the average cost of raising a child to seconda...   \n",
       "197         197  thai police investigating the murder of two br...   \n",
       "198         198  from clumpy flat shoes that seem to shorten a ...   \n",
       "199         199  for most people, a trip to the shops involves ...   \n",
       "\n",
       "                                            highlights  \\\n",
       "0    manchester united were beaten 2-0 in their cha...   \n",
       "1    rachael neustadt's sons - daniel, eight and jo...   \n",
       "2    islamic voters allegedly told to be 'good musl...   \n",
       "3    mohammed abu khder, 16, abducted and burned to...   \n",
       "4    spearmint rhino records £2.1m loss in 2011 .lo...   \n",
       "..                                                 ...   \n",
       "195  'the block' caught out faking a visit from the...   \n",
       "196  average cost of raising a child from birth up ...   \n",
       "197  pornprasit sukdam claims he was offered £13,30...   \n",
       "198  clumpy flat shoes that seem to shorten a woman...   \n",
       "199  moose spent seven hours dashing through the fi...   \n",
       "\n",
       "                                           id  \\\n",
       "0    5b3a626078390cb0e05327b4019753fd11cb8cea   \n",
       "1    59d478d4a4299e2192997e56a9db9003fa2bac2d   \n",
       "2    ec961b7d0912e7753dffe4360b77481eba96f2e1   \n",
       "3    092d90d61eb105b3955820cc4894ac2c4995ad1b   \n",
       "4    d0d59018cdf48aaeb6e1838c0323f8555e800765   \n",
       "..                                        ...   \n",
       "195  985b1bf7fc710e4ffdd9dd02e72d889a7997e89d   \n",
       "196  e466296e19d7a14cf4916d70a2cbc296e4659c99   \n",
       "197  a07624a84fe59a3321e83f153d6fd615207a8545   \n",
       "198  e16474b52bbf45f49434fc4a0b1d68e2d3fba3c3   \n",
       "199  6f4bbcea20c6d6132b5d987912e06b5f6099a6ea   \n",
       "\n",
       "                                          t5_summaries  \\\n",
       "0    manchester united lost 1-0 to olympiacos in th...   \n",
       "1    rachael neustadt's sons daniel, eight, and jon...   \n",
       "2    supporters of mayor lutfur rahman allegedly ha...   \n",
       "3    mohammed abu khder, 16, was abducted from his ...   \n",
       "4    spearmint rhino has recorded a loss of £2.1mil...   \n",
       "..                                                 ...   \n",
       "195  reality tv show the block has been accused of ...   \n",
       "196  average cost of raising a child to secondary s...   \n",
       "197  pornprasit sukdam claims he was offered 700,00...   \n",
       "198  kim carillo says she feels surprisingly sexy i...   \n",
       "199  runaway elk caught dashing through streets of ...   \n",
       "\n",
       "                               t5_fine_tuned_summaries  \n",
       "0    manchester united lost 1-0 to olympiacos in th...  \n",
       "1    rachael neustadt's sons daniel, eight, and jon...  \n",
       "2    supporters of mayor lutfur rahman allegedly ha...  \n",
       "3    mohammed abu khder, 16, was burned to death in...  \n",
       "4    spearmint rhino has recorded a loss of £2.1m i...  \n",
       "..                                                 ...  \n",
       "195  reality tv show the block has been accused of ...  \n",
       "196  average cost of raising a child to secondary s...  \n",
       "197  pornprasit sukdam claims he was offered 700,00...  \n",
       "198  kim carillo, who usually favours a more alluri...  \n",
       "199  runaway elk caught dashing through streets of ...  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/data_frame_after_t5.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e692e3a-f5b9-435e-a1f6-ea448bd09113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
